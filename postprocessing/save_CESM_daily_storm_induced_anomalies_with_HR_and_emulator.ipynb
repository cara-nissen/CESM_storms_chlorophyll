{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad5fbc2e-f891-4261-9e91-23e0cee673d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate daily anomalies in a 1000km radius around the storm centers identified with Tempest Extremes\n",
    "# store the results as netcdf files\n",
    "#\n",
    "# STEPS:\n",
    "#  for each day, find relevant storm centers\n",
    "#  get the area within 1000km around the storm center\n",
    "#  a) at each location within this area, subtract the daily climatology 1997-2018 (non-seasonal variability is left)\n",
    "#  b) at each location within this area, subtract the average over the 5 preceeding days (\"storm-induced anomalies\")\n",
    "#\n",
    "# Notes: \n",
    "#  sea-ice area is ignored (all grid cells with sea-ice concentration >0 are masked)\n",
    "#  if _plus_XX_days is processed, the same 5 days are used for averaging as when the original storm positions/timings are used as input\n",
    "#\n",
    "# original script name: save_CESM_daily_chl_anomalies_v12_with_HR_and_emulator.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb7d5f16-a930-4b60-bbf5-45c0cc4c83cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset, MFDataset\n",
    "import numba as nb\n",
    "import time as timing\n",
    "from numba import njit \n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "from geopy.distance import distance\n",
    "import seawater as sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc44ac53-3366-48fa-a240-83c820f49480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53876it [00:00, 360010.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min/max lon: -179.4375 180.0\n",
      "44258 44258 44258 44258\n",
      "Number of storms: 9618\n",
      "\n",
      "min/max year: 1997 2019\n",
      "min/max month: 1 12\n",
      "min/max day: 1 31\n",
      "min/max hour: 12 12\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#----\n",
    "# load storm tracks from tempest extremes\n",
    "#----\n",
    "\n",
    "pathTE = '/global/cfs/cdirs/m4003/cnissen/TempestExtremes_output/'\n",
    "\n",
    "time_string   = '1997_2018'  # '1979_2018' '2009_2018', 1979_1988, 2009_2015\n",
    "region_string = '4080S'\n",
    "\n",
    "#storm_string = 'all_at_noon'\n",
    "storm_string = 'all_at_noon_plus_4_days'\n",
    "\n",
    "if len(storm_string)>12: # shifted time\n",
    "    if storm_string[12]=='p':\n",
    "        days_shift = int(storm_string[-6:-5]) # make sure to use the exact same 5 days in the calculation as for \"day of passing\"\n",
    "    else: \n",
    "        days_shift = 0 # for \"minus XX days\", use the 5 preceeding days from that time\n",
    "else:\n",
    "    days_shift = 0 # no shift\n",
    "print('Time is shifted by this number of days:',days_shift)\n",
    "\n",
    "fileTE = 'SH_stormtraj_'+time_string+'_'+region_string+'_'+storm_string[4:]+'.txt'\n",
    "\n",
    "#---\n",
    "# read all storm positions for year1\n",
    "#---\n",
    "\n",
    "year_storm  = []\n",
    "month_storm = []\n",
    "day_storm   = []\n",
    "hour_storm  = []\n",
    "index_lon   = []\n",
    "index_lat   = []\n",
    "lon_storm   = []\n",
    "lat_storm   = []\n",
    "index_storm = []\n",
    "slp_center  = []\n",
    "wind_max    = []\n",
    "count_storms = []\n",
    "duration_storm = []\n",
    "counter = 0\n",
    "with open(pathTE+fileTE, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        aux = line.split()\n",
    "        if aux[0]!='start': #year1: # starting a new time entry\n",
    "            # 262     77      147.375000      -46.333780      1.003353e+05    1.493876e+01    2012    1       2       21\n",
    "            index_lon   += [int(aux[0])]\n",
    "            index_lat   += [int(aux[1])]\n",
    "            if float(aux[2])>180: # convert from 0:360 to -180:180\n",
    "                aux1 = float(aux[2])-360\n",
    "            else:\n",
    "                aux1 = float(aux[2])\n",
    "            index_storm += [int(aux[10])]\n",
    "            lon_storm   += [aux1] #[float(aux[2])]\n",
    "            lat_storm   += [float(aux[3])]\n",
    "            year_storm  += [int(aux[6])]\n",
    "            month_storm += [int(aux[7])]\n",
    "            day_storm   += [int(aux[8])]\n",
    "            hour_storm  += [int(aux[9])]\n",
    "            slp_center  += [float(aux[4])]\n",
    "            wind_max    += [float(aux[5])]\n",
    "            count_storms+= [counter]\n",
    "        else:\n",
    "            # start   29      2012    1       15      21\n",
    "            duration_storm+= [int(aux[1])]\n",
    "            counter=counter+1\n",
    "        del aux\n",
    "        \n",
    "# convert from 0:360 to -180:180\n",
    "#lon_storm[lon_storm>180] = lon_storm[lon_storm>180]-360\n",
    "print('min/max lon:',np.min(lon_storm),np.max(lon_storm))\n",
    "\n",
    "print(len(lon_storm),len(year_storm),len(month_storm),len(slp_center))\n",
    "print('Number of storms:',np.max(count_storms))\n",
    "#print('avg. duration of storms:',np.mean(duration_storm)) # -> incorrect becaue I reduce storm track to all time steps at noon\n",
    "\n",
    "print()\n",
    "print('min/max year:',np.min(year_storm),np.max(year_storm))\n",
    "print('min/max month:',np.min(month_storm),np.max(month_storm))\n",
    "print('min/max day:',np.min(day_storm),np.max(day_storm))\n",
    "print('min/max hour:',np.min(hour_storm),np.max(hour_storm))\n",
    "\n",
    "print ('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5812b72b-b7c1-465b-8b96-51e038c05004",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320,) (640,) (365,)\n",
      "-179.4375 180.0\n",
      "-89.57009 89.57009\n",
      "1980-01-01 00:00:00 1980-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#---\n",
    "# get lat/lon\n",
    "#---\n",
    "\n",
    "path1 = '/global/cfs/cdirs/m4003/cnissen/CESM_chl/'\n",
    "file_chl = 'totChl_JRA_grid_1980-01-01.nc'\n",
    "\n",
    "ff2  = xr.open_dataset(path1+file_chl)\n",
    "lat  = ff2['latitude'].values \n",
    "lon  = ff2['longitude'].values\n",
    "time = ff2['time'].values\n",
    "ff2.close()\n",
    "\n",
    "# convert from 0:360 to -180:180\n",
    "lon[lon>180] = lon[lon>180]-360\n",
    "\n",
    "print(lat.shape,lon.shape,time.shape)\n",
    "print(np.min(lon),np.max(lon))\n",
    "print(np.min(lat),np.max(lat))\n",
    "print(np.min(time),np.max(time)) \n",
    "# -> time in file suggests each day is centred around midnight, but I suspect it should be centered around noon instead!\n",
    "# (note that I added this to the file!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aa6f01f-b4fd-4c49-b568-5490832773d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-359.4375    0.5625]\n",
      "[0.5569153  0.56020355 0.5609436  0.5612259  0.5613632  0.5614395\n",
      " 0.5614929  0.5615158  0.5615387  0.56155396 0.5615616  0.56157684\n",
      " 0.5615845  0.5615921  0.56159973 0.56160736 0.561615   0.5616188\n",
      " 0.5616207  0.56162167 0.5616217  0.5616218  0.5616219  0.56162214\n",
      " 0.5616226 ]\n",
      "(640,) (320,)\n",
      "(641,) (321,)\n",
      "50% done\n",
      "Global area: 509373217207890.1 m2\n",
      "(320, 640)\n"
     ]
    }
   ],
   "source": [
    "#---\n",
    "# get area for JRA mesh\n",
    "#---\n",
    "\n",
    "# get resolution of JRA data\n",
    "print(np.unique(np.diff(lon)))\n",
    "print(np.unique(np.diff(lat)))\n",
    "\n",
    "# need the box boundaries for area calculation\n",
    "res = 0.5625\n",
    "xi = np.arange(0,360+res,res)\n",
    "yi = np.arange(-90,90+res,res)\n",
    "print(lon.shape,lat.shape)\n",
    "print(xi.shape,yi.shape)\n",
    "\n",
    "area = np.zeros((len(xi)-1,len(yi)-1))\n",
    "#calculate area\n",
    "for i in range(0,len(xi)-1): #laenge pruefen!\n",
    "    if i==np.round(len(xi)/2): \n",
    "        print ('50% done')\n",
    "    for j in range(0,len(yi)-1): \n",
    "        dist1 = sw.dist([yi[j],yi[j+1]],[xi[i],xi[i]])\n",
    "        dist2 = sw.dist([yi[j],yi[j]],[xi[i],xi[i+1]])\n",
    "        area[i][j] = float(dist1[0]) * float(dist2[0]) *1000 *1000 #m2\n",
    "area = area.transpose()\n",
    "print ('Global area:',np.sum(area),'m2') \n",
    "print(area.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8dfabb-a53c-401f-84f9-5dbbad751c78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---\n",
    "# FUNCTIONS\n",
    "#---\n",
    "\n",
    "@njit\n",
    "def get_closest_grid_point(lon_point, lat_point, lon2, lat2):  \n",
    "    # in all nodes in mesh, return the index of the closest node to lon_point/lat_point\n",
    "    # lon2 & lat2 are the locations in the new mesh (to be redistributed to)\n",
    "    # lon2 & lat2 should be in radians\n",
    "    # numpy needs to be imported outside the function\n",
    "    \n",
    "    #from math import sin, cos, sqrt, atan2, radians\n",
    "    #import numpy as np\n",
    "    # approximate radius of earth in km\n",
    "    R = 6371.0\n",
    "    \n",
    "    #lat2 = radians(mesh.y2) # all positions in mesh\n",
    "    #lon2 = radians(mesh.x2)\n",
    "    #lat2 = [radians(x) for x in mesh.y2]\n",
    "    #lon2 = [radians(x) for x in mesh.x2]\n",
    "    lat1 = radians(lat_point)\n",
    "    lon1 = radians(lon_point)\n",
    "    bb1 = cos(lat1)\n",
    "    \n",
    "    all_distances = np.zeros(len(lon2))\n",
    "    for i in range(0,len(lon2)):\n",
    "        dlon = lon2[i] - lon1\n",
    "        dlat = lat2[i] - lat1\n",
    "        a = sin(dlat / 2)**2 + bb1 * cos(lat2[i]) * sin(dlon / 2)**2\n",
    "        all_distances[i] = 2*R*atan2(sqrt(a), sqrt(1 - a)) \n",
    "    index_closest_node = np.argmin(all_distances)\n",
    "    distance_closest_node = np.min(all_distances)\n",
    "\n",
    "    return index_closest_node, distance_closest_node\n",
    "\n",
    "def mask_chl_field(points_filtered,lat,lon,data_mask):\n",
    "        # project all points within XX km of storm center back onto JRA mesh \n",
    "        # -> create mask to be used for averaging input fields, e.g., chl\n",
    "        for kk in range(0,len(points_filtered)):\n",
    "            index1 = np.where(lon==points_filtered[kk][1])[0]\n",
    "            index2 = np.where(lat==points_filtered[kk][0])[0]\n",
    "            data_mask[index2,index1] = 1\n",
    "        return data_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aba45194-e9e2-404b-826a-2bce056e945b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---\n",
    "# OPTIONAL: store \"time\" in original daily files\n",
    "#---\n",
    "# in regridding, the time variable was not correctly stored\n",
    "# for each variable, the below code had to be run once so that the time variable is correct in file\n",
    "\n",
    "add_time_to_file = False\n",
    "\n",
    "if add_time_to_file:\n",
    "    save_netcdf = True\n",
    "    fv = -999\n",
    "\n",
    "    vari = 'MLD' # FG_CO2_2 or totChl or ECOSYS_IFRAC_2\n",
    "\n",
    "    if vari in ['totChl']:\n",
    "        path_daily_mean   = '/global/cfs/cdirs/m4003/cnissen/CESM_chl/'\n",
    "    elif vari in ['FG_CO2_2']:\n",
    "        path_daily_mean   = '/global/cfs/cdirs/m4003/cnissen/CESM_fco2/'\n",
    "    elif vari in ['ECOSYS_IFRAC_2']:\n",
    "        path_daily_mean   = '/global/cfs/cdirs/m4003/cnissen/CESM_sic/'\n",
    "    elif vari in ['MLD']:\n",
    "        path_daily_mean   = '/global/cfs/cdirs/m4003/cnissen/CESM_MLD_regridded/'\n",
    "    elif vari in ['SST']:\n",
    "        path_daily_mean   = '/global/cfs/cdirs/m4003/cnissen/CESM_SST_regridded/'\n",
    "    elif vari in ['wind_speed']:\n",
    "        path_daily_mean   = '/global/cfs/cdirs/m4003/cnissen/JRA_wind_speed/'\n",
    "    elif vari in ['photoC_total_surf']:\n",
    "        path_daily_mean   = '/global/cfs/cdirs/m4003/cnissen/CESM_total_srf_photoC_regridded/'\n",
    "    elif vari in ['cloudfrac_isccp']:\n",
    "        path_daily_mean   = '/global/cfs/cdirs/m4003/cnissen/CESM_cloudfrac_isccp_regridded/'\n",
    "    elif vari in ['PAR_incoming']:\n",
    "        path_daily_mean   = '/global/cfs/cdirs/m4003/cnissen/JRA_PAR_incoming/'\n",
    "\n",
    "    year_list2 = np.arange(1979,2018+1) #9+1,1)\n",
    "    time_all   = np.arange(0,365*len(year_list2)+1,1)\n",
    "    \n",
    "    \n",
    "    #year_list2 = np.arange(1978,1978+1,1)#1979,2018+1) #9+1,1)\n",
    "    #time_all   = np.arange(-365,-1+1,1) #np.arange(0,365*len(year_list2)+1,1)\n",
    "\n",
    "    for yy in tqdm(range(17,len(year_list2))):\n",
    "\n",
    "     #   # daily data\n",
    "     #   file1 = 'totChl_JRA_grid_'+str(year_list2[yy])+'-01-01.nc'\n",
    "     #   ff = xr.open_dataset(path_daily_mean+file1)\n",
    "     #   data1 = np.squeeze(ff['totChl']).values\n",
    "     #   lat = np.squeeze(ff['latitude']).values\n",
    "     #   lon = np.squeeze(ff['longitude']).values\n",
    "     #   ff.close()\n",
    "\n",
    "        # store in new file\n",
    "        if save_netcdf:\n",
    "            time = time_all[yy*365:(yy+1)*365]\n",
    "            netcdf_name = vari+'_JRA_grid_'+str(year_list2[yy])+'-01-01.nc'\n",
    "            w_nc_fid = Dataset(path_daily_mean+netcdf_name, 'r+', format='NETCDF4_CLASSIC')\n",
    "            try: # if time variable does not exist, define it first\n",
    "                w_nc_var1 = w_nc_fid.createVariable('time', 'f4',('time'),fill_value=fv)\n",
    "                w_nc_var1.units = 'days since '+str(year_list2[0])+'-01-01 00:00:00'\n",
    "                w_nc_var1.calendar = 'noleap'  \n",
    "            except: \n",
    "                pass\n",
    "            w_nc_fid.variables['time'][:] = time\n",
    "            w_nc_fid.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13682c0a-c379-49b6-aa02-c87cb664c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totChl_emulator totChl ...\n",
      "Load year 2013\n",
      "min/max ice cover: 1.2223171e-16 1.0\n",
      "Get number of storms for current year...\n",
      "min/max/median/mean storms per day: 2.0 11.0 6.0 5.789041095890411\n",
      "Sum of storms for current year: 2113.0\n",
      "Create file /global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/totChl_emulator_anomalies/Anomalies_within_1000km_of_storm_center_totChl_emulator_JRA_grid_2013-01-01_all_at_noon_plus_4_days_subtract_clim_first_gap_filled_clim.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/365 [00:00<?, ?it/s]/global/homes/c/cnissen/.conda/envs/myenv/lib/python3.9/site-packages/numba/core/ir_utils.py:2152: NumbaPendingDeprecationWarning: \n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'lat2' of function 'get_closest_grid_point'.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\n",
      "File \"../../../../../tmp/ipykernel_558905/3917996291.py\", line 5:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n",
      "/global/homes/c/cnissen/.conda/envs/myenv/lib/python3.9/site-packages/numba/core/ir_utils.py:2152: NumbaPendingDeprecationWarning: \n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'lon2' of function 'get_closest_grid_point'.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\n",
      "File \"../../../../../tmp/ipykernel_558905/3917996291.py\", line 5:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n",
      "<timed exec>:544: RuntimeWarning: Mean of empty slice\n",
      "100%|██████████| 365/365 [1:15:10<00:00, 12.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load year 2014\n",
      "min/max ice cover: 8.2041245e-16 1.0\n",
      "Get number of storms for current year...\n",
      "min/max/median/mean storms per day: 1.0 10.0 5.0 5.5917808219178085\n",
      "Sum of storms for current year: 2041.0\n",
      "Create file /global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/totChl_emulator_anomalies/Anomalies_within_1000km_of_storm_center_totChl_emulator_JRA_grid_2014-01-01_all_at_noon_plus_4_days_subtract_clim_first_gap_filled_clim.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 365/365 [1:12:48<00:00, 11.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load year 2015\n",
      "min/max ice cover: 1.1540176e-15 1.0\n",
      "Get number of storms for current year...\n",
      "min/max/median/mean storms per day: 1.0 11.0 5.0 5.517808219178082\n",
      "Sum of storms for current year: 2014.0\n",
      "Create file /global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/totChl_emulator_anomalies/Anomalies_within_1000km_of_storm_center_totChl_emulator_JRA_grid_2015-01-01_all_at_noon_plus_4_days_subtract_clim_first_gap_filled_clim.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 365/365 [1:11:17<00:00, 11.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load year 2016\n",
      "min/max ice cover: 6.940603e-16 1.0\n",
      "Get number of storms for current year...\n",
      "min/max/median/mean storms per day: 0.0 11.0 6.0 5.671232876712328\n",
      "Sum of storms for current year: 2070.0\n",
      "Create file /global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/totChl_emulator_anomalies/Anomalies_within_1000km_of_storm_center_totChl_emulator_JRA_grid_2016-01-01_all_at_noon_plus_4_days_subtract_clim_first_gap_filled_clim.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 365/365 [1:13:23<00:00, 12.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load year 2017\n",
      "min/max ice cover: 7.819186e-16 1.0\n",
      "Get number of storms for current year...\n",
      "min/max/median/mean storms per day: 2.0 12.0 6.0 5.556164383561644\n",
      "Sum of storms for current year: 2028.0\n",
      "Create file /global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/totChl_emulator_anomalies/Anomalies_within_1000km_of_storm_center_totChl_emulator_JRA_grid_2017-01-01_all_at_noon_plus_4_days_subtract_clim_first_gap_filled_clim.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 365/365 [1:12:08<00:00, 11.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load year 2018\n",
      "min/max ice cover: 1.8208962e-16 1.0\n",
      "Get number of storms for current year...\n",
      "min/max/median/mean storms per day: 0.0 11.0 6.0 5.8054794520547945\n",
      "Sum of storms for current year: 2119.0\n",
      "Create file /global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/totChl_emulator_anomalies/Anomalies_within_1000km_of_storm_center_totChl_emulator_JRA_grid_2018-01-01_all_at_noon_plus_4_days_subtract_clim_first_gap_filled_clim.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 365/365 [1:14:15<00:00, 12.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5h 1min 49s, sys: 2h 12min 31s, total: 7h 14min 20s\n",
      "Wall time: 7h 19min 44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#---\n",
    "# LOAD DATA\n",
    "#---\n",
    "\n",
    "# define years; important: load one year more here to correcty treat the beginning of the first year of interest!\n",
    "year_list = np.arange(int(time_string[0:4])-1,int(time_string[5:9])+1,1) \n",
    "\n",
    "# optional test plots (These were used in development of script, i.e., for a single day of a single year. If these are set to true, a lot of plots are created...)\n",
    "test_plot_masked_field  = False\n",
    "test_plot_combined_mask = False\n",
    "test_plot_mask          = False\n",
    "\n",
    "save_netcdf = True\n",
    "\n",
    "#----\n",
    "# choose a list of variables to be processed\n",
    "#----\n",
    "\n",
    "vari_list = ['totChl','totChl_emulator','totChl_hr']\n",
    "# ALL VARIABLES: \n",
    "# totChl, totChl_hr, totChl_emulator\n",
    "# PAR_incoming, MLD, MLD_hr, wind_speed, SST, slp, cloudfrac_isccp\n",
    "# diat_specific_growth_rate_surf, sp_specific_growth_rate_surf, photoC_total_surf, photoC_zint\n",
    "# sp_Fe_lim_surf, sp_N_lim_surf, sp_P_lim_surf\n",
    "# diat_Fe_lim_surf, diat_N_lim_surf, diat_P_lim_surf, diat_SiO3_lim_surf\n",
    "\n",
    "# \"storm-induced anomalies\" or \"non-seasonal variability\"?\n",
    "clim_only = False # if True, step a) only (see header); if False, both step a) and b)\n",
    "\n",
    "#----\n",
    "# define distance thresholds\n",
    "#----\n",
    "dist_threshold     = 1000 # find all points within this distance of the storm center (in km)\n",
    "dist_threshold_deg = 25   # search radius within which to check whether JRA points fulfill dist_threshold criterion (decreases run time of script)\n",
    "\n",
    "# loop over variables\n",
    "for vv in range(0,len(vari_list)):\n",
    "    if vari_list[vv] in ['diat_specific_growth_rate_surf']:\n",
    "        vari = 'mu_diat'\n",
    "    elif vari_list[vv] in ['sp_specific_growth_rate_surf']:\n",
    "        vari = 'mu_sp'\n",
    "    elif vari_list[vv] in ['totChl_emulator']:\n",
    "        vari = 'totChl'\n",
    "    elif vari_list[vv] in ['totChl_hr']:\n",
    "        vari = 'totChl'\n",
    "    elif vari_list[vv] in ['MLD_hr']:\n",
    "        vari = 'HMXL_2'\n",
    "    else:\n",
    "        vari = vari_list[vv] \n",
    "    print(vari_list[vv],vari,'...')\n",
    "    \n",
    "    if vari in ['totChl']:\n",
    "        if vari_list[vv] in ['totChl_hr']:\n",
    "            path_string = 'CESM_HIGH_RES_chl_regridded'\n",
    "            long_name   = 'total chlorophyll'\n",
    "            unit        = 'mg chl m-3'\n",
    "        elif vari_list[vv] in ['totChl_emulator']:\n",
    "            path_string = 'CESM_totChl_emulator_regridded'\n",
    "            long_name   = 'total chlorophyll'\n",
    "            unit        = 'mg chl m-3'\n",
    "        else:\n",
    "            path_string = 'CESM_chl'\n",
    "            long_name   = 'total chlorophyll'\n",
    "            unit        = 'mg chl m-3'\n",
    "    elif vari in ['FG_CO2_2']:\n",
    "        path_string = 'CESM_fco2'\n",
    "        long_name   = 'air-sea CO2 flux'\n",
    "        unit        = 'mmol m-3 cm s-1'\n",
    "    elif vari in ['photoC_total_surf']:\n",
    "        path_string = 'CESM_total_srf_photoC_regridded'\n",
    "        long_name   = 'total surface NPP'\n",
    "        unit        = 'mmol m-3 cm s-1'\n",
    "    elif vari in ['SST']:\n",
    "        path_string = 'CESM_SST_regridded'\n",
    "        long_name   = 'Sea surface temperature (potential)'\n",
    "        unit        = 'deg C'\n",
    "    elif vari in ['MLD']:\n",
    "        path_string = 'CESM_MLD_regridded'\n",
    "        long_name   = 'mixed layer depth (density-based)'\n",
    "        unit        = 'm'\n",
    "    elif vari in ['MLD_hr','HMXL_2']:\n",
    "        path_string = 'CESM_HIGH_RES_MLD_regridded'\n",
    "        long_name   = 'mixed layer depth (density-based)'\n",
    "        unit        = 'm'\n",
    "    elif vari in ['wind_speed']:\n",
    "        path_string = 'JRA_wind_speed'\n",
    "        long_name   = '10m wind speed'\n",
    "        unit        = 'm s-1'\n",
    "    elif vari in ['diat_Fe_lim_surf']:\n",
    "        path_string = 'CESM_diat_Fe_lim_surf_regridded'\n",
    "        long_name   = 'diatom surface iron limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diat_N_lim_surf']:\n",
    "        path_string = 'CESM_diat_N_lim_surf_regridded'\n",
    "        long_name   = 'diatom surface nitrate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diat_P_lim_surf']:\n",
    "        path_string = 'CESM_diat_P_lim_surf_regridded'\n",
    "        long_name   = 'diatom surface phosphate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diat_SiO3_lim_surf']:\n",
    "        path_string = 'CESM_diat_SiO3_lim_surf_regridded'\n",
    "        long_name   = 'diatom surface silicate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['sp_Fe_lim_surf']:\n",
    "        path_string = 'CESM_sp_Fe_lim_surf_regridded'\n",
    "        long_name   = 'SP surface iron limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['sp_N_lim_surf']:\n",
    "        path_string = 'CESM_sp_N_lim_surf_regridded'\n",
    "        long_name   = 'SP surface nitrate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['sp_P_lim_surf']:\n",
    "        path_string = 'CESM_sp_P_lim_surf_regridded'\n",
    "        long_name   = 'SP surface phosphate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diatChl_SURF']:\n",
    "        path_string = 'CESM_diatChl_SURF_regridded'\n",
    "        long_name   = 'diatom surface chlorophyll'\n",
    "        unit        = 'mg chl m-3'\n",
    "    elif vari in ['spChl_SURF']:\n",
    "        path_string = 'CESM_spChl_SURF_regridded'\n",
    "        long_name   = 'SP surface chlorophyll'\n",
    "        unit        = 'mg chl m-3'\n",
    "    elif vari in ['diat_light_lim_surf']:\n",
    "        path_string = 'CESM_diat_light_lim_surf_regridded'\n",
    "        long_name   = 'diatom surface light limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['sp_light_lim_surf']:\n",
    "        path_string = 'CESM_sp_light_lim_surf_regridded'\n",
    "        long_name   = 'SP surface light limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diat_specific_growth_rate_surf','mu_diat']:\n",
    "        path_string = 'CESM_diat_specific_growth_rate_regridded'\n",
    "        long_name   = 'diatom surface specific growth rate'\n",
    "        unit        = 'd-1'\n",
    "    elif vari in ['sp_specific_growth_rate_surf','mu_sp']:\n",
    "        path_string = 'CESM_sp_specific_growth_rate_regridded'\n",
    "        long_name   = 'SP surface specific growth rate'\n",
    "        unit        = 'd-1'\n",
    "    elif vari in ['cloudfrac_isccp']:\n",
    "        path_string = 'CESM_cloudfrac_isccp_regridded'\n",
    "        long_name   = 'ISCCP cloud fraction'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['PAR_incoming']:\n",
    "        path_string = 'JRA_PAR_incoming'\n",
    "        long_name   = 'Incoming PAR (45% of incoming shortwave radiation)'\n",
    "        unit        = 'W m-2'\n",
    "    elif vari in ['photoC_zint']:\n",
    "        path_string = 'CESM_total_int_photoC_regridded'\n",
    "        long_name   = 'total vertically integrated NPP absolute field'\n",
    "        unit        = 'mmol m-3 cm s-1 m2'\n",
    "    elif vari in ['slp']:\n",
    "        path_string = 'JRA_slp'\n",
    "        long_name   = 'sea level pressure'\n",
    "        unit        = 'Pa'        \n",
    "        \n",
    "    # path to data\n",
    "    path1 = '/global/cfs/cdirs/m4003/cnissen/'+path_string+'/'\n",
    "\n",
    "    #---\n",
    "    # load daily clim \n",
    "    #---\n",
    "    if vari_list[vv] in ['diat_specific_growth_rate_surf','sp_specific_growth_rate_surf','totChl_hr','MLD_hr']:\n",
    "        file_clim = 'Climatology_'+vari_list[vv]+'_JRA_grid_1997_2018.nc' \n",
    "    #elif vari_list[vv] in ['totChl_emulator']: # if gap-filled climatology, use this\n",
    "    #    file_clim = 'Climatology_'+vari_list[vv]+'_JRA_grid_1997_2018_interpolated.nc' \n",
    "    else:\n",
    "        file_clim = 'Climatology_'+vari+'_JRA_grid_1997_2018.nc' \n",
    "    if vari_list[vv] in ['totChl_emulator']:\n",
    "        ff2  = xr.open_dataset('/global/cfs/cdirs/m4003/cnissen/CESM_chl/'+file_clim) # load full-field climatology\n",
    "        #ff2  = xr.open_dataset('/global/cfs/cdirs/m4003/cnissen/CESM_totChl_emulator_regridded/'+file_clim) # load gap-filled climatology\n",
    "    else:\n",
    "        ff2  = xr.open_dataset(path1+file_clim)\n",
    "    if vari_list[vv] in ['totChl_emulator']: # if gap filled, use this\n",
    "        data_clim = ff2['totChl_masked']#.values \n",
    "    elif vari_list[vv]=='MLD_hr':\n",
    "        data_clim = ff2['MLD_hr']\n",
    "    else:\n",
    "        data_clim = ff2[vari]#.values \n",
    "    ff2.close()\n",
    "    data_clim2 = np.concatenate((data_clim,data_clim))\n",
    "\n",
    "    # where to save anomaly files?\n",
    "    if vari_list[vv] in ['totChl_emulator']:\n",
    "        savepath     = '/global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/'+vari+'_emulator_anomalies/'\n",
    "    elif vari_list[vv] in ['totChl_hr']:\n",
    "        savepath     = '/global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/'+vari+'_hr_anomalies/'\n",
    "    elif vari_list[vv]=='MLD_hr':\n",
    "        savepath     = '/global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/MLD_hr_anomalies/'\n",
    "    else:\n",
    "        savepath     = '/global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/'+vari+'_anomalies/'\n",
    "    # check existence of paths\n",
    "    if not os.path.exists(savepath):\n",
    "        print ('Created '+savepath)\n",
    "        os.makedirs(savepath)\n",
    "    \n",
    "    for yy in range(1,len(year_list)):   # START AT INDEX \"1\" HERE \n",
    "                                         # (I always load \"yy-1\" as well which won't work if I start with yy=0 here!)\n",
    "\n",
    "        print('Load year',year_list[yy])\n",
    "        if vari_list[vv] in ['diat_specific_growth_rate_surf','sp_specific_growth_rate_surf','totChl_hr','MLD_hr']:\n",
    "            file1 = vari_list[vv]+'_JRA_grid_'+str(year_list[yy-1])+'-01-01.nc'\n",
    "            file2 = vari_list[vv]+'_JRA_grid_'+str(year_list[yy])+'-01-01.nc'\n",
    "        elif vari_list[vv] in ['totChl_emulator']:\n",
    "            file1 = vari+'_emulator_JRA_grid_'+str(year_list[yy-1])+'-01-01.nc'\n",
    "            file2 = vari+'_emulator_JRA_grid_'+str(year_list[yy])+'-01-01.nc'\n",
    "        else:\n",
    "            file1 = vari+'_JRA_grid_'+str(year_list[yy-1])+'-01-01.nc'\n",
    "            file2 = vari+'_JRA_grid_'+str(year_list[yy])+'-01-01.nc'\n",
    "        \n",
    "        # previous year\n",
    "        ff2  = xr.open_dataset(path1+file1)\n",
    "        if vari_list[vv] in ['totChl_emulator']:\n",
    "            chl1 = ff2['totChl_masked']#.values \n",
    "        else:\n",
    "            chl1 = ff2[vari]#.values \n",
    "        time1= ff2['time'].values \n",
    "        ff2.close()\n",
    "        # current year\n",
    "        ff2  = xr.open_dataset(path1+file2)\n",
    "        if vari_list[vv] in ['totChl_emulator']:\n",
    "            chl2 = ff2['totChl_masked']#.values \n",
    "        else:\n",
    "            chl2 = ff2[vari]#.values \n",
    "        time2= ff2['time'].values \n",
    "        ff2.close()\n",
    "        # merge the two years\n",
    "        chl_all  = np.concatenate((chl1,chl2))\n",
    "        time_all = np.concatenate((time1,time2))\n",
    "        \n",
    "        # create land-sea mask\n",
    "        land_sea_mask = np.ones_like(chl_all[0,:,:])\n",
    "        land_sea_mask[np.isnan(chl_all[0,:,:])] = 0\n",
    "\n",
    "        #----\n",
    "        # load sea-ice data\n",
    "        path2 = '/global/cfs/cdirs/m4003/cnissen/CESM_sic/'\n",
    "        file1 = 'ECOSYS_IFRAC_2_JRA_grid_'+str(year_list[yy])+'-01-01.nc'\n",
    "        ff  = xr.open_dataset(path2+file1)\n",
    "        data_ice = ff['ECOSYS_IFRAC_2'].values \n",
    "        print('min/max ice cover:',np.nanmin(data_ice),np.nanmax(data_ice))\n",
    "        ff.close()\n",
    "        # # mask chl at \"high\" sea-ice cover further down\n",
    "        # ice_threshold = 0.5\n",
    "        #----\n",
    "\n",
    "        #---\n",
    "        # count storms on each day\n",
    "        #---\n",
    "        # NOTE: the summed number resulting here is NOT the same as count_storms above!\n",
    "        #  count_storms is for all years loaded in\n",
    "        #  additionally, here, a storm is counted multiple times if it exists over multiple days\n",
    "        #\n",
    "        # the number resulting here (np.sum(num_storms_each_day)) is the size of the array to be defined for netcdf file\n",
    "\n",
    "        print('Get number of storms for current year...')\n",
    "        num_storms_each_day = np.zeros(365)\n",
    "        for t1 in range(0,365): # loop over days in year\n",
    "\n",
    "            tt = t1+365 # I loaded two years of data -> always process the 2nd one (to always be able to go back by 10 days!)\n",
    "\n",
    "            aux_year  = time_all[tt].year\n",
    "            aux_month = time_all[tt].month\n",
    "            aux_day   = time_all[tt].day\n",
    "            aux_hour  = 12 # always at noon (center of daily averages)\n",
    "            ind_find_storm = np.where((np.asarray(year_storm)==aux_year) &\\\n",
    "                                          (np.asarray(month_storm)==aux_month) &\\\n",
    "                                          (np.asarray(day_storm)==aux_day) &\\\n",
    "                                          (np.asarray(hour_storm)==aux_hour))[0]\n",
    "            num_storms_each_day[t1] = len(ind_find_storm)\n",
    "            del aux_year,aux_month,aux_day,aux_hour\n",
    "\n",
    "        print('min/max/median/mean storms per day:',np.min(num_storms_each_day),np.max(num_storms_each_day),\\\n",
    "                  np.median(num_storms_each_day),np.mean(num_storms_each_day))\n",
    "        print('Sum of storms for current year:',np.sum(num_storms_each_day))\n",
    "\n",
    "        #---\n",
    "        # create netcdf file\n",
    "        #---\n",
    "        \n",
    "        which_clim = '_full_field_clim' #'_gap_filled_clim' # needs to be changed for gap-filled climatology for emulator!\n",
    "        if save_netcdf:\n",
    "            fv = -999\n",
    "            if clim_only: \n",
    "                clim_string = '_clim_only'\n",
    "            else:\n",
    "                clim_string = '_clim_first'\n",
    "                \n",
    "            if vari_list[vv] in ['totChl_emulator']:\n",
    "                netcdf_name = 'Anomalies_within_'+str(dist_threshold)+'km_of_storm_center_'+\\\n",
    "                                    vari+'_emulator_JRA_grid_'+str(year_list[yy])+'-01-01_'+storm_string+'_subtract'+clim_string+which_clim+'.nc'\n",
    "            elif vari_list[vv] in ['totChl_hr']:\n",
    "                netcdf_name = 'Anomalies_within_'+str(dist_threshold)+'km_of_storm_center_'+\\\n",
    "                                    vari+'_hr_JRA_grid_'+str(year_list[yy])+'-01-01_'+storm_string+'_subtract'+clim_string+'.nc'\n",
    "            else:\n",
    "                netcdf_name = 'Anomalies_within_'+str(dist_threshold)+'km_of_storm_center_'+\\\n",
    "                                    vari+'_JRA_grid_'+str(year_list[yy])+'-01-01_'+storm_string+'_subtract'+clim_string+'.nc'\n",
    "            if not os.path.exists(savepath+netcdf_name):\n",
    "                print('Create file '+savepath+netcdf_name)\n",
    "                w_nc_fid = Dataset(savepath+netcdf_name, 'w', format='NETCDF4_CLASSIC')\n",
    "                w_nc_fid.contact     = 'Cara Nissen, cara.nissen@colorado.edu'\n",
    "                w_nc_fid.source_data = path1+file2\n",
    "                w_nc_fid.storm_file  = pathTE+fileTE\n",
    "                w_nc_fid.script      = '/global/homes/c/cnissen/scripts/save_CESM_daily_chl_anomalies_v12_with_HR_and_emulator.ipynb'\n",
    "                # create dimension & variable\n",
    "                w_nc_fid.createDimension('lon', len(lon)) \n",
    "                w_nc_fid.createDimension('lat', len(lat)) \n",
    "                w_nc_fid.createDimension('count_anom', np.sum(num_storms_each_day)) \n",
    "                #w_nc_fid.createDimension('time', time_all[365:,:,:].shape[0]) \n",
    "\n",
    "                w_nc_var1 = w_nc_fid.createVariable(vari+'_storm', 'f4',('count_anom','lat','lon'),fill_value=fv)\n",
    "                w_nc_var1.long_name = long_name+' absolute field'\n",
    "                w_nc_var1.units = unit\n",
    "                w_nc_var1 = w_nc_fid.createVariable(vari+'_storm_anomaly4', 'f4',('count_anom','lat','lon'),fill_value=fv)\n",
    "                w_nc_var1.long_name = long_name+' anomaly (subtracted daily climatology at each location)'\n",
    "                w_nc_var1.units = unit\n",
    "                w_nc_var1 = w_nc_fid.createVariable('sea_ice', 'f4',('count_anom','lat','lon'),fill_value=fv)\n",
    "                w_nc_var1.long_name = 'sea ice concentration'\n",
    "                w_nc_var1.units = 'n.d.'\n",
    "                w_nc_var1 = w_nc_fid.createVariable('lat', 'f4',('lat'),fill_value=fv)\n",
    "                w_nc_var1.description = 'Latitude'\n",
    "                w_nc_var1.units = 'deg N'\n",
    "                w_nc_var1 = w_nc_fid.createVariable('lon', 'f4',('lon'),fill_value=fv)\n",
    "                w_nc_var1.description = 'Longitude (-180:180)'\n",
    "                w_nc_var1.units = 'deg E'\n",
    "                w_nc_var1 = w_nc_fid.createVariable('land_sea_mask', 'f4',('lat','lon'),fill_value=fv)\n",
    "                w_nc_var1.long_name = 'Land-sea mask'\n",
    "                #----\n",
    "                # store some extra info about current storm\n",
    "                w_nc_var1 = w_nc_fid.createVariable('index_storm','f4',('count_anom'),fill_value=fv)\n",
    "                w_nc_var1.description = 'index of storm (to know which storm imprints belong to the same storm)'\n",
    "                w_nc_var1 = w_nc_fid.createVariable('max_wind_storm','f4',('count_anom'),fill_value=fv)\n",
    "                w_nc_var1.description = 'max. wind speed in m s-1 at current time step'\n",
    "                w_nc_var1 = w_nc_fid.createVariable('min_slp_storm','f4',('count_anom'),fill_value=fv)\n",
    "                w_nc_var1.description = 'min. SLP speed in Pa at current time step'\n",
    "                w_nc_var1 = w_nc_fid.createVariable('lon_storm','f4',('count_anom'),fill_value=fv)\n",
    "                w_nc_var1.description = 'Longitude of storm center'\n",
    "                w_nc_var1 = w_nc_fid.createVariable('lat_storm','f4',('count_anom'),fill_value=fv)\n",
    "                w_nc_var1.description = 'Latitude of storm center'\n",
    "                w_nc_var1 = w_nc_fid.createVariable('year_storm','f4',('count_anom'),fill_value=fv)\n",
    "                w_nc_var1.description = 'Year of storm center'\n",
    "                w_nc_var1 = w_nc_fid.createVariable('month_storm','f4',('count_anom'),fill_value=fv)\n",
    "                w_nc_var1.description = 'Month of storm center'\n",
    "                w_nc_var1 = w_nc_fid.createVariable('day_storm','f4',('count_anom'),fill_value=fv)\n",
    "                w_nc_var1.description = 'Day of storm center'\n",
    "                w_nc_var1 = w_nc_fid.createVariable('hour_storm','f4',('count_anom'),fill_value=fv)\n",
    "                w_nc_var1.description = 'Hour of storm center'\n",
    "                #----\n",
    "                \n",
    "                #w_nc_var1 = w_nc_fid.createVariable('time', 'f4',('time'),fill_value=fv)\n",
    "                #w_nc_var1.units = 'days since '+str(year_list2[0])+'-01-01 00:00:00'\n",
    "                #w_nc_var1.calendar = 'noleap'   \n",
    "\n",
    "                # write lat/lon to file\n",
    "                w_nc_fid.variables['land_sea_mask'][:] = land_sea_mask\n",
    "                w_nc_fid.variables['lat'][:] = lat\n",
    "                w_nc_fid.variables['lon'][:] = lon\n",
    "\n",
    "                w_nc_fid.close()\n",
    "\n",
    "        #----\n",
    "        # get all points within 1000km, create a mask to be used for averaging, get avg CHL within that mask for the 10 days preceding the storm day -> calculate anomaly\n",
    "        #----\n",
    "        # see: https://gis.stackexchange.com/questions/450878/finding-all-latitude-and-longitudes-within-x-km-range-from-input-location-using\n",
    "\n",
    "        storm_count = 0 # used for indexing when writing netcdf\n",
    "        for t1 in tqdm(range(0,365)): # loop over days in year\n",
    "\n",
    "            tt = t1+365 # I loaded two years of data -> always process the 2nd one (to always be able to go back by 10 days!)\n",
    "\n",
    "            aux_year  = time_all[tt].year\n",
    "            aux_month = time_all[tt].month\n",
    "            aux_day   = time_all[tt].day\n",
    "            aux_hour  = 12 # always at noon (center of daily averages)\n",
    "\n",
    "            ind_find_storm = np.where((np.asarray(year_storm)==aux_year) &\\\n",
    "                                      (np.asarray(month_storm)==aux_month) &\\\n",
    "                                      (np.asarray(day_storm)==aux_day) &\\\n",
    "                                      (np.asarray(hour_storm)==aux_hour))[0]\n",
    "        #    print('Number of storms:',len(ind_find_storm))\n",
    "            del aux_year,aux_month,aux_day,aux_hour\n",
    "\n",
    "            if len(ind_find_storm)>0: # if any storm was found, continue\n",
    "\n",
    "                # extract all storm positions\n",
    "                aux_index    = np.asarray(index_storm)[ind_find_storm]\n",
    "                aux_min_slp  = np.asarray(slp_center)[ind_find_storm]\n",
    "                aux_max_wind = np.asarray(wind_max)[ind_find_storm]\n",
    "                aux_lon = np.asarray(lon_storm)[ind_find_storm]\n",
    "                aux_lat = np.asarray(lat_storm)[ind_find_storm]\n",
    "                aux_year  = np.asarray(year_storm)[ind_find_storm]\n",
    "                aux_month = np.asarray(month_storm)[ind_find_storm]\n",
    "                aux_day   = np.asarray(day_storm)[ind_find_storm]\n",
    "                aux_hour   = np.asarray(hour_storm)[ind_find_storm]\n",
    "\n",
    "                data_anom_all_storms = np.zeros_like(chl_all[0,:,:])\n",
    "                mask_all_storms      = np.zeros_like(chl_all[0,:,:])\n",
    "                for ss in range(0,len(ind_find_storm)): # loop over storms on current day\n",
    "\n",
    "                    lon_point, lat_point = aux_lon[ss],aux_lat[ss] # current storm position\n",
    "                    lon2,lat2 = np.meshgrid(lon,lat) # JRA mesh\n",
    "                    # reduce to array to within +-XX° of current storm location of interest (for 1000km, this should be enough)\n",
    "                    dist_threshold_deg2 = 5\n",
    "                    ind_lat = np.where((lat>(lat_point-dist_threshold_deg2)) & (lat<(lat_point+dist_threshold_deg2)))[0]\n",
    "                    ind_lon = np.where((lon>(lon_point-dist_threshold_deg2)) & (lon<(lon_point+dist_threshold_deg2)))[0]\n",
    "                    lat2 = lat2[ind_lat,:][:,ind_lon]\n",
    "                    lon2 = lon2[ind_lat,:][:,ind_lon]\n",
    "                    lon2,lat2 = lon2.ravel(),lat2.ravel()\n",
    "                    # find closest node in reduced JRA mesh\n",
    "                    lat2_rad = [radians(x) for x in lat2]\n",
    "                    lon2_rad = [radians(x) for x in lon2]\n",
    "                    index_closest_node, distance_closest_node = get_closest_grid_point(lon_point, lat_point,\\\n",
    "                                                                                       lon2_rad,lat2_rad)\n",
    "                    # get corresponding indices in full JRA mesh\n",
    "                    aux1 = lon2[index_closest_node]\n",
    "                    aux2 = lat2[index_closest_node]\n",
    "                    mm1 = np.where(lat==aux2)[0]\n",
    "                    nn1 = np.where(lon==aux1)[0]\n",
    "                    del aux1,aux2,ind_lat,ind_lon,lat2,lon2,index_closest_node,distance_closest_node\n",
    "\n",
    "                    # get mask on JRA mesh for all points within XXkm of storm position\n",
    "                #    print('lon/lat of storm at current time:',lon[nn1],lat[mm1])\n",
    "                    lon_point,lat_point = lon[nn1],lat[mm1]\n",
    "                    input_point = (lat_point, lon_point)\n",
    "\n",
    "                    # input points: reorganize grid cells of JRA mesh: \n",
    "                    lon2,lat2 = np.meshgrid(lon,lat)\n",
    "                    # reduce to array to within +-XX° of current storm location of interest (for 1000km, this should be enough)\n",
    "                    # lon runs from -180:180, i.e., there is a jump from -180 to +180 at the dateline in the Pacific!\n",
    "                    # for all lon_point close enough to the dateline (within dist_threshold_deg), the search for \n",
    "                    #      indices needs to be adapted!\n",
    "                    if lon_point<(-180+dist_threshold_deg): # storm is to the right of the dateline (within dist_threshold_deg)\n",
    "                        ind_lon = np.where(((lon>(lon_point-dist_threshold_deg)) & (lon<(lon_point+dist_threshold_deg))) | (lon>(180-dist_threshold_deg-(180+lon_point))))[0]\n",
    "                    elif lon_point>(180-dist_threshold_deg): # storm is to the left of the dateline (within dist_threshold_deg)\n",
    "                        ind_lon = np.where(((lon>(lon_point-dist_threshold_deg)) & (lon<(lon_point+dist_threshold_deg))) | (lon<(-180+dist_threshold_deg-(180-lon_point))))[0]\n",
    "                    else: # distance to dateline is >dist_threshold_deg\n",
    "                        ind_lon = np.where((lon>(lon_point-dist_threshold_deg)) & (lon<(lon_point+dist_threshold_deg)))[0]\n",
    "                    ind_lat = np.where((lat>(lat_point-dist_threshold_deg)) & (lat<(lat_point+dist_threshold_deg)))[0]    \n",
    "                    lat2 = lat2[ind_lat,:][:,ind_lon]\n",
    "                    lon2 = lon2[ind_lat,:][:,ind_lon]\n",
    "                    #print(lat2.shape)\n",
    "                    points_jra = []\n",
    "                    for mm in range(0,lat2.shape[0]):\n",
    "                        for nn in range(0,lon2.shape[1]):\n",
    "                            aux = (lat2[mm,nn],lon2[mm,nn])\n",
    "                            points_jra.append(aux)\n",
    "                            del aux\n",
    "\n",
    "                    # loop over all JRA points in the area and get all points that fulfill distance criterion\n",
    "                    points_filtered = []\n",
    "                    for point in points_jra:\n",
    "                        if distance(input_point, point).km < dist_threshold:\n",
    "                            points_filtered.append(point)\n",
    "\n",
    "                    del points_jra,lat2,lon2,input_point\n",
    "\n",
    "                    lon2,lat2 = np.meshgrid(lon,lat) # re-define to get the size of the full JRA mesh\n",
    "                    lon2[lon2<0] = lon2[lon2<0]+360\n",
    "                    chl_mask  = np.zeros_like(lat2) # initialize data_mask (to be used in averaging)\n",
    "                    chl_mask  = mask_chl_field(points_filtered,lat,lon,chl_mask)\n",
    "                    chl_mask    = np.ma.masked_where(np.isnan(chl_all[0,:,:]),chl_mask) # apply land-sea mask\n",
    "\n",
    "                    if test_plot_mask:\n",
    "                        fig  = plt.figure(figsize=(5,2.5))\n",
    "                        plt.pcolor(lon2,lat2,chl_mask,shading='auto')\n",
    "                        # plt.plot(nn1,mm1,'wo')\n",
    "                        if lon[nn1]<0:\n",
    "                            lon_plot = lon[nn1]+360\n",
    "                        else:\n",
    "                            lon_plot = lon[nn1]\n",
    "                        plt.plot(lon_plot,lat[mm1],'bx')\n",
    "                        plt.show()\n",
    "                        del lon_plot\n",
    "\n",
    "                    #----\n",
    "                    # get chl avg within \"storm-impacted region\" for the 10 preceeding days\n",
    "                    #----           \n",
    "                    aux_chl = chl_all[tt-10:tt,:,:] # reduce to 10 preceeding days\n",
    "\n",
    "                    area_masked = np.copy(area)\n",
    "                    area_masked = np.ma.masked_where(np.isnan(chl_all[0,:,:]),area_masked) # apply land-sea mask\n",
    "                    area_masked = np.tile(area_masked,[10,1,1]) # bring to the same size as aux_chl\n",
    "\n",
    "                    aux_chl = np.multiply(aux_chl,chl_mask) # 10 x lat x lon\n",
    "                    #print('aux_chl.shape',aux_chl.shape)\n",
    "                 #   aux_chl_avg_10 = np.nanmean(aux_chl,axis=0)\n",
    "                 #   aux_chl_avg_7  = np.nanmean(aux_chl[3:,:,:],axis=0)\n",
    "                 #   aux_chl_avg_5  = np.nanmean(aux_chl[5:,:,:],axis=0)\n",
    "\n",
    "                    if test_plot_masked_field:\n",
    "                        print('aux_index[ss]:',aux_index[ss])\n",
    "                        # calculate anomaly in \"storm-impacted region\"\n",
    "                        chl_current = np.copy(chl_all)[tt,:,:]\n",
    "                        \n",
    "                        indSO = np.where(lat<-30)[0]\n",
    "                        \n",
    "                        fig  = plt.figure(figsize=(8,2.5))\n",
    "                        plt.pcolor(lon2[indSO,:],lat2[indSO,:],chl_current[indSO,:],cmap=plt.cm.RdYlBu_r,vmin=0,vmax=100,shading='auto')\n",
    "                        plt.colorbar()\n",
    "                        plt.show()\n",
    "                        \n",
    "                        #chl_current[chl_mask==1] = chl_current[chl_mask==1]-chl_avg_5 # within 1000km of storm center, subtract mean field of the preceeding 10 days\n",
    "                        chl_current[chl_mask==1] = chl_current[chl_mask==1]\n",
    "                        chl_current[chl_mask==0] = 0 # outside of storm-impacted area, set field to zero\n",
    "                        if vari in ['totChl']:\n",
    "                            cmap1 = plt.cm.viridis\n",
    "                        elif vari in ['FG_CO2_2']:\n",
    "                            cmap1 = plt.cm.RdBu_r\n",
    "                            max1  = np.max(np.abs(aux_chl[0,:,:]))\n",
    "                        else:\n",
    "                            cmap1 = plt.cm.RdYlBu_r\n",
    "                        fig  = plt.figure(figsize=(8,2.5))\n",
    "                        if vari in ['totChl']:\n",
    "                            plt.pcolor(lon2[indSO,:],lat2[indSO,:],chl_current[indSO,:],cmap=cmap1,shading='auto')\n",
    "                        elif vari in ['FG_CO2_2']:\n",
    "                            plt.pcolor(lon2[indSO,:],lat2[indSO,:],chl_current[indSO,:],cmap=cmap1,vmin=-0.012,vmax=0.012,shading='auto') #vmin=-1*max1,vmax=max1\n",
    "                        else:\n",
    "                            plt.pcolor(lon2[indSO,:],lat2[indSO,:],chl_current[indSO,:],cmap=cmap1,vmin=0,vmax=100,shading='auto')\n",
    "                        plt.colorbar()\n",
    "                        if lon[nn1]<0:\n",
    "                            lon_plot = lon[nn1]+360\n",
    "                        else:\n",
    "                            lon_plot = lon[nn1]\n",
    "                        plt.plot(lon_plot,lat[mm1],'bx')\n",
    "                        plt.show()\n",
    "                    #    del lon_plot,chl_current,indSO\n",
    "\n",
    "                    if save_netcdf:\n",
    "                       # absolute field: \n",
    "                        chl_current_abs  = np.copy(chl_all)[tt,:,:]\n",
    "                        chl_current_abs[chl_mask==1] = chl_current_abs[chl_mask==1]\n",
    "                        chl_current_abs[chl_mask==0] = fv # outside of storm-impacted area, set field to fill value\n",
    "                        chl_current_abs[chl_mask.mask==True] = fv\n",
    "                        #-----\n",
    "                        # anomaly 4: CORRECTION -> subtract clim, then calculate avg for 5 days before from this anomaly (not the original abs. field)!\n",
    "                        chl_current4 = np.copy(chl_all)[tt,:,:]\n",
    "                        chl_current4[chl_mask==1] = chl_current4[chl_mask==1]-(data_clim[tt-365,:,:].values)[chl_mask==1] # subtract daily clim.\n",
    "                        if not clim_only:\n",
    "                            #-------\n",
    "                            # NEW VERSION: \n",
    "                            #  always subtract the same 5 days, no matter if +1, +2 etc is chosen!\n",
    "                            aux_chl2 = chl_all[tt-5-days_shift:tt-days_shift,:,:] # reduce to 5 preceeding days\n",
    "                            aux_chl2 = np.multiply(aux_chl2,chl_mask) # 5 x lat x lon\n",
    "                            for dd in range(0,5): # get anomaly for each of the days preceding the storm\n",
    "                                aux_chl2[dd,:,:] = aux_chl2[dd,:,:]-data_clim2[tt-5-days_shift:tt-days_shift,:,:][dd,:,:] # subtract daily clim.#-data_clim[tt-365,:,:]\n",
    "                            aux_chl_avg2 = np.nanmean(aux_chl2,axis=0)\n",
    "                            # subtract this from the anomaly\n",
    "                            chl_current4[chl_mask==1] = chl_current4[chl_mask==1]-aux_chl_avg2[chl_mask==1] # within 1000km of storm center, subtract climatology at each location, then subtract avg of 10 prec. days at each location\n",
    "                            #-------\n",
    "                           # #---\n",
    "                           # # COMMENT FROM HERE TO \"END\" for subtract_clim_only\n",
    "                           # # get avg for the 5 days preceding the storm\n",
    "                           # aux_chl2 = chl_all[tt-5:tt,:,:] # reduce to 5 preceeding days\n",
    "                           # aux_chl2 = np.multiply(aux_chl2,chl_mask) # 5 x lat x lon\n",
    "                           # for dd in range(0,5): # get anomaly for each of the days preceding the storm\n",
    "                           #     aux_chl2[dd,:,:] = aux_chl2[dd,:,:]-data_clim2[tt-5:tt,:,:][dd,:,:] # subtract daily clim.#-data_clim[tt-365,:,:]\n",
    "                           # aux_chl_avg2 = np.nanmean(aux_chl2,axis=0)\n",
    "                           # # subtract this from the anomaly\n",
    "                           # chl_current4[chl_mask==1] = chl_current4[chl_mask==1]-aux_chl_avg2[chl_mask==1] # within 1000km of storm center, subtract climatology at each location, then subtract avg of 10 prec. days at each location\n",
    "                           # # END\n",
    "                           # #------\n",
    "                        chl_current4[chl_mask==0] = fv # outside of storm-impacted area, set field to fill value\n",
    "                        chl_current4[chl_mask.mask==True] = fv\n",
    "                      #  del aux_chl2\n",
    "                        #------\n",
    "                        \n",
    "                        # sea ice\n",
    "                        ice_aux = np.copy(data_ice)[tt-365,:,:] # only loaded current year\n",
    "                        ice_aux[chl_mask==0] = fv # outside of storm-impacted area, set field to fill value\n",
    "\n",
    "                        w_nc_fid = Dataset(savepath+netcdf_name, 'r+', format='NETCDF4_CLASSIC') \n",
    "                        w_nc_fid.variables[vari+'_storm'][storm_count,:,:]          = chl_current_abs\n",
    "                        w_nc_fid.variables[vari+'_storm_anomaly4'][storm_count,:,:] = chl_current4 # subtract avg of 5 prec. days at each location\n",
    "                        w_nc_fid.variables['sea_ice'][storm_count]     = ice_aux\n",
    "                        w_nc_fid.variables['index_storm'][storm_count] = aux_index[ss]\n",
    "                        w_nc_fid.variables['max_wind_storm'][storm_count] = aux_max_wind[ss]\n",
    "                        w_nc_fid.variables['min_slp_storm'][storm_count]  = aux_min_slp[ss]\n",
    "                        w_nc_fid.variables['lon_storm'][storm_count]   = lon_point\n",
    "                        w_nc_fid.variables['lat_storm'][storm_count]   = lat_point\n",
    "                        w_nc_fid.variables['year_storm'][storm_count]  = aux_year[ss]\n",
    "                        w_nc_fid.variables['month_storm'][storm_count] = aux_month[ss]\n",
    "                        w_nc_fid.variables['day_storm'][storm_count]   = aux_day[ss]\n",
    "                        w_nc_fid.variables['hour_storm'][storm_count]  = aux_hour[ss]\n",
    "                        #---\n",
    "                        # also store wind speed and SLP!\n",
    "                        #---\n",
    "                        w_nc_fid.close()  \n",
    "\n",
    "                        del chl_current4,ice_aux\n",
    "                        # del chl_current2,chl_current6,chl_current7,chl_current_abs\n",
    "                \n",
    "                        #print(storm_count,np.nanmax(chl_current4))\n",
    "\n",
    "                    # update storm_count\n",
    "                    storm_count = storm_count+1\n",
    "\n",
    "                    # combine all storms of a single day into one array (to think about: could there ever be overlap??? If yes, that's a problem in this step)\n",
    "                    mask_all_storms[chl_mask==1] = 1\n",
    "\n",
    "                    del ind_lat,ind_lon,lon2,lat2,points_filtered,aux_chl,chl_mask\n",
    "                del aux_lon,aux_lat,aux_year,aux_month,aux_day,aux_hour\n",
    "\n",
    "                # plot the resulting storm mask (all storms on current day)\n",
    "                if test_plot_combined_mask:\n",
    "                    lon2,lat2 = np.meshgrid(lon,lat) # re-define to get the size of the full JRA mesh\n",
    "                    lon2[lon2<0] = lon2[lon2<0]+360\n",
    "                    indSO = np.where(lat<-30)[0]\n",
    "                    fig  = plt.figure(figsize=(7,2.5))\n",
    "                    plt.pcolor(lon2[indSO,:],lat2[indSO,:],mask_all_storms[indSO,:],shading='auto')\n",
    "                    plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb55af-2443-4abd-a601-e27ff27e643a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb62e4-af77-446e-b166-03092fef90ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d45da15-3993-485d-ad5a-4bdb2caac80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6429d4d3-06fb-43a2-8a5c-d6d3915de210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79963fd-7397-4168-91c1-7803bc083a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-jupyter",
   "language": "python",
   "name": "myenv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
