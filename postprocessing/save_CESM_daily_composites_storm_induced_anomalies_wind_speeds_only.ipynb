{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5fbc2e-f891-4261-9e91-23e0cee673d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create composites \n",
    "# store as netcdf files\n",
    "# v2: take all existing time steps at noon into account for a given storm, store integrated anomaly\n",
    "#     store both an average and an integral for the variables it makes sense for \n",
    "#     integral for: chl, CO2 flux, NPP\n",
    "#     all others: store avg only\n",
    "#\n",
    "# v3: like v2 but for new calculation of anomaly (subtract clim. first)\n",
    "#\n",
    "# HERE: corrected the identification of storms (previous version had ~50 duplicates in resulting files)\n",
    "# fix: load all years at once and loop over the unique index_storm (as provided by txt file that contains the storms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7d5f16-a930-4b60-bbf5-45c0cc4c83cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset, MFDataset\n",
    "import numba as nb\n",
    "import time as timing\n",
    "from numba import njit \n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "from geopy.distance import distance\n",
    "import seawater as sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "656a1595-5147-4f7c-968a-a52201627053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---\n",
    "# FUNCTION \n",
    "#---\n",
    "\n",
    "def get_distance_to_storm_center(lat2,lon2,aux_lat,aux_lon):\n",
    "                \n",
    "    # create list of locations within 1000km of the storm\n",
    "    points_data = []\n",
    "    for pp in range(0,lat2.shape[0]):\n",
    "        aux = (lat2[pp],lon2[pp])\n",
    "        points_data.append(aux)\n",
    "        del aux\n",
    "\n",
    "    #print(len(points_data))\n",
    "    # for each of these points get the distance to the storm center in km -> get distance in x-dir and y-dir\n",
    "    points_distance_x = np.zeros(len(points_data)) # distance in longitudinal direction, i.e., use latitude of storm (aux_lat)\n",
    "    points_distance_y = np.zeros(len(points_data)) # distance in latitudinal direction, i.e., use longitude of storm (aux_lon)\n",
    "    for pp in range(0,len(points_data)): \n",
    "        # distance in longitudinal direction\n",
    "        aux_point = (aux_lat,points_data[pp][1])\n",
    "        points_distance_x[pp] = distance(point_storm, aux_point).km\n",
    "       # print(aux_point,point_storm,points_distance_x[pp])\n",
    "        # check sign: if lon grid cell is smaller (=further west) than lon of storm, define distance to be negative\n",
    "        if points_data[pp][1]<aux_lon:\n",
    "            points_distance_x[pp] = -1*points_distance_x[pp]\n",
    "        elif (aux_lon<0) & (points_data[pp][1]>0): # lon_storm is east of dateline, but grid cell is west of dateline (grid cell is also further west in this case!)\n",
    "            if (points_data[pp][1]-360)<aux_lon:\n",
    "                points_distance_x[pp] = -1*points_distance_x[pp]\n",
    "        del aux_point\n",
    "        # distance in latitudinal direction\n",
    "        aux_point = (points_data[pp][0],aux_lon)\n",
    "        points_distance_y[pp] = distance(point_storm, aux_point).km\n",
    "        # check sign: if lat grid cell is smaller (=further south) than lat of storm, define distance to be negative\n",
    "        if points_data[pp][0]<aux_lat:\n",
    "            points_distance_y[pp] = -1*points_distance_y[pp]\n",
    "        del aux_point  \n",
    "    return points_distance_x,points_distance_y,points_data\n",
    "                    \n",
    "    \n",
    "def bin_points_as_distance_to_storm_center(counter,points_distance_x,points_distance_y,x_bins,y_bins,aux_data_anom,data_storm_mean,data_storm_std,data_storm_count):\n",
    "    # data_storm_mean,data_storm_std,data_storm_count: initialized arrays, will be filled in this function and then returned\n",
    "    \n",
    "    # bin the points (account for where each point is relative to storm center)\n",
    "    ind_x = np.digitize(points_distance_x,x_bins,right=False) # minimum is 1 (not zero!!)\n",
    "    ind_y = np.digitize(points_distance_y,y_bins,right=False)\n",
    " #   print(np.min(ind_x),np.max(ind_x))\n",
    " #   print(np.min(ind_y),np.max(ind_y))\n",
    "    # returned index satisfies: bins[i-1] <= x < bins[i]\n",
    "\n",
    "   # print(aux_data_anom.shape)\n",
    "    for xx in range(1,len(x_bins)+1): # start at 1 here -> see note above for ind_x\n",
    "        for yy in range(1,len(x_bins)+1):\n",
    "            index = np.where((ind_y==yy) & (ind_x==xx))[0]\n",
    "            if len(index)>0:\n",
    "                #if counter==6: \n",
    "                #    print(xx,yy,index.shape,aux_data_anom.shape)\n",
    "                #if (counter==40) & (xx==11) & (yy==20):\n",
    "                #    print(xx,yy,len(points_distance_x))\n",
    "                #    print(index)\n",
    "                #    print(index.shape,aux_data_anom.shape)\n",
    "                # anomaly 2\n",
    "                data_storm_mean[xx-1,yy-1]  = np.nanmean(aux_data_anom[index])\n",
    "                data_storm_std[xx-1,yy-1]   = np.nanstd(aux_data_anom[index])\n",
    "                data_storm_count[xx-1,yy-1] = index.shape[0]\n",
    "            del index\n",
    "    return data_storm_mean,data_storm_std,data_storm_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b7c1b5-376d-4525-87b3-46fae921400d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process  totChl \n",
      "<xarray.DataArray 'index_storm' (count_anom: 44258)>\n",
      "dask.array<concatenate, shape=(44258,), dtype=float32, chunksize=(2126,), chunktype=numpy.ndarray>\n",
      "Dimensions without coordinates: count_anom\n",
      "Attributes:\n",
      "    description:  index of storm (to know which storm imprints belong to the ...\n",
      "\n",
      "num_storms (ALL YEARS): 9615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9615/9615 [05:08<00:00, 31.16it/s]\n"
     ]
    }
   ],
   "source": [
    "#---\n",
    "# LOAD INFO FROM ALL YEARS AT ONCE\n",
    "#----\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', message='Mean of empty slice')\n",
    "    \n",
    "save_netcdf = True\n",
    "years = np.arange(1997,2018+1,1)\n",
    "\n",
    "#vari_list = ['totChl','totChl_emulator','totChl_hr','MLD',\\\n",
    "#             'PAR_incoming','slp','photoC_total_surf']\n",
    "#vari_list = ['wind_speed','diat_specific_growth_rate_surf','sp_specific_growth_rate_surf',\\\n",
    "#            'photoC_zint','phytoC_zint_100m','SST']\n",
    "\n",
    "vari_list = ['totChl']\n",
    "time_string = ''   #'_plus_2_days'\n",
    "\n",
    "# for each year, find the number of storms\n",
    "# go through storms and get avg/integral as a function of \"distance to storm center\"\n",
    "# store the avg/integral anomaly for each storm\n",
    "# also store: number of days the storm existed, avg SLP, min SLP\n",
    "\n",
    "for vv in range(0,len(vari_list)):\n",
    "    if vari_list[vv] in ['diat_specific_growth_rate_surf','mu_diat']:\n",
    "        vari = 'mu_diat'\n",
    "    elif vari_list[vv] in ['sp_specific_growth_rate_surf','mu_sp']:\n",
    "        vari = 'mu_sp'\n",
    "    elif vari_list[vv] in ['totChl_hr']:\n",
    "        vari = 'totChl'\n",
    "    else:\n",
    "        vari = vari_list[vv] \n",
    "    #vari = vari_list[vv] #'totChl' # totChl, FG_CO2_2\n",
    "    print('Process ',vari,time_string)    \n",
    "\n",
    "    if vari in ['totChl']:\n",
    "        long_name   = 'total chlorophyll'\n",
    "        unit        = 'mg chl m-3'\n",
    "    elif vari in ['totChl_emulator']:\n",
    "        long_name   = 'total chlorophyll'\n",
    "        unit        = 'mg chl m-3'\n",
    "    elif vari in ['FG_CO2_2']:\n",
    "        long_name   = 'air-sea CO2 flux'\n",
    "        unit        = 'mmol m-3 cm s-1'\n",
    "    elif vari in ['photoC_total_surf']:\n",
    "        long_name   = 'total surface NPP'\n",
    "        unit        = 'mmol m-3 cm s-1'\n",
    "    elif vari in ['SST']:\n",
    "        long_name   = 'Sea surface temperature (potential)'\n",
    "        unit        = 'deg C'\n",
    "    elif vari in ['MLD']:\n",
    "        long_name   = 'mixed layer depth (density-based)'\n",
    "        unit        = 'm' # actually in cm in file! convert further down!\n",
    "    elif vari in ['wind_speed']:\n",
    "        long_name   = '10m wind speed'\n",
    "        unit        = 'm s-1'\n",
    "    elif vari in ['diat_Fe_lim_surf']:\n",
    "        long_name   = 'diatom surface iron limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diat_N_lim_surf']:\n",
    "        long_name   = 'diatom surface nitrate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diat_P_lim_surf']:\n",
    "        long_name   = 'diatom surface phosphate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diat_SiO3_lim_surf']:\n",
    "        long_name   = 'diatom surface silicate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['sp_Fe_lim_surf']:\n",
    "        long_name   = 'SP surface iron limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['sp_N_lim_surf']:\n",
    "        long_name   = 'SP surface nitrate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['sp_P_lim_surf']:\n",
    "        long_name   = 'SP surface phosphate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diatChl_SURF']:\n",
    "        long_name   = 'diatom surface chlorophyll'\n",
    "        unit        = 'mg chl m-3'\n",
    "    elif vari in ['spChl_SURF']:\n",
    "        long_name   = 'SP surface chlorophyll'\n",
    "        unit        = 'mg chl m-3'\n",
    "    elif vari in ['diat_light_lim_surf']:\n",
    "        long_name   = 'diatom surface light limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['sp_light_lim_surf']:\n",
    "        long_name   = 'SP surface light limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diat_specific_growth_rate_surf','mu_diat']:\n",
    "        long_name   = 'diatom surface specific growth rate'\n",
    "        unit        = 'd-1'\n",
    "    elif vari in ['sp_specific_growth_rate_surf','mu_sp']:\n",
    "        long_name   = 'SP surface specific growth rate'\n",
    "        unit        = 'd-1'\n",
    "    elif vari in ['phytoC_zint_100m']:\n",
    "        long_name   = 'total integrated phytoplankton C biomass in the top 100m'\n",
    "        unit        = 'mmol C m-3'\n",
    "    elif vari in ['photoC_zint']:\n",
    "        long_name   = 'total vertically integrated NPP'\n",
    "        unit        = 'mmol m-3 cm s-1 m2'\n",
    "    elif vari in ['cloudfrac_isccp']:\n",
    "        long_name   = 'ISCCP cloud cover'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['PAR_incoming']:\n",
    "        long_name   = 'Incoming PAR (45% of incoming shortwave radiation)'\n",
    "        unit        = 'W m-2'\n",
    "    elif vari in ['slp']:\n",
    "        long_name   = 'sea level pressure'\n",
    "        unit        = 'Pa'\n",
    "   \n",
    "    if vari_list[vv] in ['totChl_hr']:\n",
    "        path1 = '/global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/'+vari+'_hr_anomalies/'\n",
    "    else:\n",
    "        path1 = '/global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/'+vari+'_anomalies/'\n",
    "        \n",
    "    # where to save anomaly files?\n",
    "    savepath     = path1 #'/global/cfs/cdirs/m4003/cnissen/CESM_'+vari+'_anomalies/'\n",
    "    # check existence of paths\n",
    "    if not os.path.exists(savepath):\n",
    "        print ('Created '+savepath)\n",
    "        os.makedirs(savepath)\n",
    "        \n",
    "    if vari_list[vv]=='totChl_hr':\n",
    "        ds = xr.open_mfdataset(path1+'Anomalies_within_1000km_of_storm_center_'+vari+'_hr_JRA_grid_*noon'+\\\n",
    "                               time_string+'_subtract_clim_first.nc',\\\n",
    "                           concat_dim='count_anom',combine='nested')\n",
    "    else:\n",
    "        ds = xr.open_mfdataset(path1+'Anomalies_within_1000km_of_storm_center_'+vari+'_JRA_grid_*noon'+time_string+\\\n",
    "                               '_subtract_clim_first.nc',\\\n",
    "                           concat_dim='count_anom',combine='nested')\n",
    "    print(ds['index_storm'])\n",
    "    print()\n",
    "\n",
    "    res    = 100\n",
    "    x_bins = np.arange(-1000,1000+res,res)\n",
    "    y_bins = np.arange(-1000,1000+res,res)\n",
    "\n",
    "    dist_threshold = 1000\n",
    "\n",
    "    # load lat/lon  \n",
    "    if vari_list[vv] in ['totChl_hr']:\n",
    "        file1 = 'Anomalies_within_1000km_of_storm_center_'+vari+'_hr_JRA_grid_1997-01-01_all_at_noon'+\\\n",
    "                        time_string+'_subtract_clim_first.nc'\n",
    "    elif vari_list[vv] in ['totChl_emulator']:\n",
    "        file1 = 'Anomalies_within_1000km_of_storm_center_'+vari+'_JRA_grid_1997-01-01_all_at_noon'+\\\n",
    "                        time_string+'_subtract_clim_first_full_field_clim.nc'\n",
    "    else:\n",
    "        file1 = 'Anomalies_within_1000km_of_storm_center_'+vari+'_JRA_grid_1997-01-01_all_at_noon'+\\\n",
    "                        time_string+'_subtract_clim_first.nc'\n",
    "    ff2  = xr.open_dataset(path1+file1)\n",
    "    lat       = ff2['lat'].values #[0:150] # model grid\n",
    "    lon       = ff2['lon'].values # model grid\n",
    "    lat,lon = np.meshgrid(lat,lon)\n",
    "    lat = lat.transpose()\n",
    "    lon = lon.transpose()\n",
    "    ff2.close()\n",
    "\n",
    "    #index_storm = np.asarray([int(x) for x in ds['index_storm']])\n",
    "    list_storms = np.unique(ds['index_storm']) #np.asarray([int(x) for x in np.unique(ds['index_storm'])])\n",
    "    num_storms = len(np.unique(ds['index_storm']))\n",
    "    print('num_storms (ALL YEARS):',num_storms)\n",
    "\n",
    "    #---\n",
    "    # create netcdf file\n",
    "    #---\n",
    "    if save_netcdf:\n",
    "        fv = -999\n",
    "        if vari_list[vv] in ['totChl_hr']:\n",
    "            netcdf_name = 'Composite_anomalies_within_'+str(dist_threshold)+'km_of_storm_center_at_noon_'+\\\n",
    "                                vari+'_hr_'+str(years[0])+'_'+str(years[-1])+time_string+'_subtract_clim_first_wind_speeds.nc'\n",
    "        else:\n",
    "            netcdf_name = 'Composite_anomalies_within_'+str(dist_threshold)+'km_of_storm_center_at_noon_'+\\\n",
    "                                vari+'_'+str(years[0])+'_'+str(years[-1])+time_string+'_subtract_clim_first_wind_speeds.nc'\n",
    "        if not os.path.exists(savepath+netcdf_name):\n",
    "            print('Create file '+savepath+netcdf_name)\n",
    "            w_nc_fid = Dataset(savepath+netcdf_name, 'w', format='NETCDF4_CLASSIC')\n",
    "            w_nc_fid.contact     = 'Cara Nissen, cara.nissen@colorado.edu'\n",
    "            w_nc_fid.source_data = path1+file1\n",
    "            w_nc_fid.script      = '/global/homes/c/cnissen/scripts/save_CESM_daily_chl_anomalies_composites_v3_subtract_clim_first_CORRECTED_max_wind_speeds_only.ipynb'\n",
    "            w_nc_fid.sea_ice     = 'sea ice area is masked and not considered in composites'\n",
    "            # create dimension & variable\n",
    "            w_nc_fid.createDimension('x_bins', len(x_bins)) \n",
    "            w_nc_fid.createDimension('y_bins', len(y_bins)) \n",
    "            w_nc_fid.createDimension('num_storms', num_storms) \n",
    "            #w_nc_fid.createDimension('time', time_all[365:,:,:].shape[0]) \n",
    "      \n",
    "            w_nc_var1 = w_nc_fid.createVariable('x_bins', 'f4',('x_bins'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Bins in x-direction (lon); '\n",
    "            w_nc_var1 = w_nc_fid.createVariable('y_bins', 'f4',('y_bins'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Bins in y-direction (lat); '\n",
    "\n",
    "            w_nc_var1 = w_nc_fid.createVariable('lon_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Longitude of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('lat_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Latitude of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('year_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Year of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('month_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Month of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('day_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Day of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('hour_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Hour of storm center at min. SLP'\n",
    "\n",
    "            w_nc_var1 = w_nc_fid.createVariable('avg_max_wind_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Average max wind speed for each storm (averaged over each min. SLP at noon)'\n",
    "            w_nc_var1.units = 'm s-1'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('max_max_wind_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Maximum maximum wind speed during the existence of each storm (based on min. SLP at noon)'\n",
    "            w_nc_var1.units = 'm s-1'\n",
    "                \n",
    "            # write lat/lon to file\n",
    "            w_nc_fid.variables['x_bins'][:]  = x_bins\n",
    "            w_nc_fid.variables['y_bins'][:]  = y_bins\n",
    "\n",
    "            w_nc_fid.close()\n",
    "\n",
    "    # loop over storms\n",
    "    counter = 0\n",
    "    for ss in tqdm(list_storms): #range(0,num_storms)):\n",
    "\n",
    "        ind_storm  = np.where(ds['index_storm']==ss)[0] \n",
    "        #print(ind_storm.shape[0])\n",
    "        aux_array = ds['min_slp_storm'][ind_storm].values\n",
    "\n",
    "        # additional information to be stored in file\n",
    "        days_storm = ind_storm.shape[0] # duration of the storm -> to be stored in the file\n",
    "        max_wind_avg_storm = np.mean(ds['max_wind_storm'][ind_storm]) # avg max. wind for all considered time steps -> to be stored in the file\n",
    "        max_wind_max_storm = np.min(ds['max_wind_storm'][ind_storm]) # max. max. wind for all considered time steps -> to be stored in the fil\n",
    "        \n",
    "        if save_netcdf:\n",
    "                \n",
    "            w_nc_fid = Dataset(savepath+netcdf_name, 'r+', format='NETCDF4_CLASSIC') \n",
    "                        \n",
    "            w_nc_fid.variables['avg_max_wind_storm'][counter] = max_wind_avg_storm\n",
    "            w_nc_fid.variables['max_max_wind_storm'][counter] = max_wind_max_storm\n",
    "                \n",
    "            w_nc_fid.close()  \n",
    "        \n",
    "        counter = counter+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07a09d5b-2c7a-48a8-aa88-da4d71174518",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite_anomalies_within_1000km_of_storm_center_at_noon_totChl_1997_2018_subtract_clim_first_wind_speeds.nc\n"
     ]
    }
   ],
   "source": [
    "print(netcdf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c02ad2-4b0d-4f8f-a925-57b4d96b4a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36fe0a4-a6e4-4a96-a9c3-95457f48169c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-jupyter",
   "language": "python",
   "name": "myenv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
