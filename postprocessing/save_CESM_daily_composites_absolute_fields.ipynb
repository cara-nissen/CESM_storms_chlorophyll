{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad5fbc2e-f891-4261-9e91-23e0cee673d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create composites fo storm-induced anomalies \n",
    "# HERE: composite absolute fields of the chosen variables\n",
    "# note that a lot of additional information is stored in the resulting netcdf files, such as number of days the storm existed, min SLP etc\n",
    "#\n",
    "# HERE (Nov 2024): corrected the identification of storms compared to an earlier version (previous version had ~50 duplicates in resulting files)\n",
    "# fix: load all years at once and loop over the unique index_storm (as provided by txt file that contains the storms)\n",
    "#\n",
    "# original script name: save_CESM_daily_chl_fields_composites_v3_subtract_clim_first_CORRECTED-.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb7d5f16-a930-4b60-bbf5-45c0cc4c83cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset, MFDataset\n",
    "import numba as nb\n",
    "import time as timing\n",
    "from numba import njit \n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "from geopy.distance import distance\n",
    "import seawater as sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656a1595-5147-4f7c-968a-a52201627053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---\n",
    "# FUNCTION \n",
    "#---\n",
    "\n",
    "def get_distance_to_storm_center(lat2,lon2,aux_lat,aux_lon):\n",
    "                \n",
    "    # create list of locations within 1000km of the storm\n",
    "    points_data = []\n",
    "    for pp in range(0,lat2.shape[0]):\n",
    "        aux = (lat2[pp],lon2[pp])\n",
    "        points_data.append(aux)\n",
    "        del aux\n",
    "\n",
    "    #print(len(points_data))\n",
    "    # for each of these points get the distance to the storm center in km -> get distance in x-dir and y-dir\n",
    "    points_distance_x = np.zeros(len(points_data)) # distance in longitudinal direction, i.e., use latitude of storm (aux_lat)\n",
    "    points_distance_y = np.zeros(len(points_data)) # distance in latitudinal direction, i.e., use longitude of storm (aux_lon)\n",
    "    for pp in range(0,len(points_data)): \n",
    "        # distance in longitudinal direction\n",
    "        aux_point = (aux_lat,points_data[pp][1])\n",
    "        points_distance_x[pp] = distance(point_storm, aux_point).km\n",
    "       # print(aux_point,point_storm,points_distance_x[pp])\n",
    "        # check sign: if lon grid cell is smaller (=further west) than lon of storm, define distance to be negative\n",
    "        if points_data[pp][1]<aux_lon:\n",
    "            points_distance_x[pp] = -1*points_distance_x[pp]\n",
    "        elif (aux_lon<0) & (points_data[pp][1]>0): # lon_storm is east of dateline, but grid cell is west of dateline (grid cell is also further west in this case!)\n",
    "            if (points_data[pp][1]-360)<aux_lon:\n",
    "                points_distance_x[pp] = -1*points_distance_x[pp]\n",
    "        del aux_point\n",
    "        # distance in latitudinal direction\n",
    "        aux_point = (points_data[pp][0],aux_lon)\n",
    "        points_distance_y[pp] = distance(point_storm, aux_point).km\n",
    "        # check sign: if lat grid cell is smaller (=further south) than lat of storm, define distance to be negative\n",
    "        if points_data[pp][0]<aux_lat:\n",
    "            points_distance_y[pp] = -1*points_distance_y[pp]\n",
    "        del aux_point  \n",
    "    return points_distance_x,points_distance_y,points_data\n",
    "                    \n",
    "    \n",
    "def bin_points_as_distance_to_storm_center(counter,points_distance_x,points_distance_y,x_bins,y_bins,aux_data_anom,data_storm_mean):\n",
    "    # data_storm_mean,data_storm_std,data_storm_count: initialized arrays, will be filled in this function and then returned\n",
    "    \n",
    "    # bin the points (account for where each point is relative to storm center)\n",
    "    ind_x = np.digitize(points_distance_x,x_bins,right=False) # minimum is 1 (not zero!!)\n",
    "    ind_y = np.digitize(points_distance_y,y_bins,right=False)\n",
    " #   print(np.min(ind_x),np.max(ind_x))\n",
    " #   print(np.min(ind_y),np.max(ind_y))\n",
    "    # returned index satisfies: bins[i-1] <= x < bins[i]\n",
    "\n",
    "   # print(aux_data_anom.shape)\n",
    "    for xx in range(1,len(x_bins)+1): # start at 1 here -> see note above for ind_x\n",
    "        for yy in range(1,len(x_bins)+1):\n",
    "            index = np.where((ind_y==yy) & (ind_x==xx))[0]\n",
    "            if len(index)>0:\n",
    "                #if counter==6: \n",
    "                #    print(xx,yy,index.shape,aux_data_anom.shape)\n",
    "                #if (counter==40) & (xx==11) & (yy==20):\n",
    "                #    print(xx,yy,len(points_distance_x))\n",
    "                #    print(index)\n",
    "                #    print(index.shape,aux_data_anom.shape)\n",
    "                # anomaly 2\n",
    "                data_storm_mean[xx-1,yy-1]  = np.nanmean(aux_data_anom[index])\n",
    "               # data_storm_std[xx-1,yy-1]   = np.nanstd(aux_data_anom[index])\n",
    "               # data_storm_count[xx-1,yy-1] = index.shape[0]\n",
    "            del index\n",
    "    return data_storm_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b7c1b5-376d-4525-87b3-46fae921400d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process  totChl _plus_4_days\n",
      "<xarray.DataArray 'index_storm' (count_anom: 44211)>\n",
      "dask.array<concatenate, shape=(44211,), dtype=float32, chunksize=(2119,), chunktype=numpy.ndarray>\n",
      "Dimensions without coordinates: count_anom\n",
      "Attributes:\n",
      "    description:  index of storm (to know which storm imprints belong to the ...\n",
      "\n",
      "num_storms (ALL YEARS): 9614\n",
      "Create file /global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/totChl_hr_anomalies/Composite_abs_field_within_1000km_of_storm_center_at_noon_totChl_hr_1997_2018_plus_4_days_subtract_clim_first.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9614/9614 [3:30:00<00:00,  1.31s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#---\n",
    "# LOAD INFO FROM ALL YEARS AT ONCE\n",
    "#----\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', message='Mean of empty slice')\n",
    "    \n",
    "save_netcdf = True\n",
    "years = np.arange(1997,2018+1,1)\n",
    "\n",
    "vari_list = ['totChl','totChl_emulator','totChl_hr']\n",
    "# ALL VARIABLES: \n",
    "# totChl, totChl_hr, totChl_emulator\n",
    "# PAR_incoming, MLD, MLD_hr, wind_speed, SST, slp, cloudfrac_isccp\n",
    "# diat_specific_growth_rate_surf, sp_specific_growth_rate_surf, photoC_total_surf, photoC_zint\n",
    "# sp_Fe_lim_surf, sp_N_lim_surf, sp_P_lim_surf\n",
    "# diat_Fe_lim_surf, diat_N_lim_surf, diat_P_lim_surf, diat_SiO3_lim_surf\n",
    "\n",
    "# time_string_list: this needs to be of the same length as vari_list\n",
    "time_string_list = [''] #,'_plus_4_days','_plus_2_days','_plus_2_days','_plus_2_days']\n",
    "\n",
    "# for each year, find the number of storms\n",
    "# go through storms and get avg/integral as a function of \"distance to storm center\"\n",
    "# store the avg/integral anomaly for each storm\n",
    "# also store: number of days the storm existed, avg SLP, min SLP\n",
    "\n",
    "for vv in range(0,len(vari_list)):\n",
    "    time_string = time_string_list[vv]\n",
    "    if vari_list[vv] in ['diat_specific_growth_rate_surf','mu_diat']:\n",
    "        vari = 'mu_diat'\n",
    "    elif vari_list[vv] in ['sp_specific_growth_rate_surf','mu_sp']:\n",
    "        vari = 'mu_sp'\n",
    "    elif vari_list[vv] in ['totChl_hr']:\n",
    "        vari = 'totChl'\n",
    "    else:\n",
    "        vari = vari_list[vv] \n",
    "    #vari = vari_list[vv] #'totChl' # totChl, FG_CO2_2\n",
    "    print('Process ',vari,time_string)    \n",
    "\n",
    "    if vari in ['totChl']:\n",
    "        long_name   = 'total chlorophyll'\n",
    "        unit        = 'mg chl m-3'\n",
    "    elif vari in ['totChl_emulator']:\n",
    "        long_name   = 'total chlorophyll'\n",
    "        unit        = 'mg chl m-3'\n",
    "    elif vari in ['FG_CO2_2']:\n",
    "        long_name   = 'air-sea CO2 flux'\n",
    "        unit        = 'mmol m-3 cm s-1'\n",
    "    elif vari in ['photoC_total_surf']:\n",
    "        long_name   = 'total surface NPP'\n",
    "        unit        = 'mmol m-3 cm s-1'\n",
    "    elif vari in ['SST']:\n",
    "        long_name   = 'Sea surface temperature (potential)'\n",
    "        unit        = 'deg C'\n",
    "    elif vari in ['MLD']:\n",
    "        long_name   = 'mixed layer depth (density-based)'\n",
    "        unit        = 'm' # actually in cm in file! convert further down!\n",
    "    elif vari in ['MLD_hr']:\n",
    "        long_name   = 'mixed layer depth (density-based)'\n",
    "        unit        = 'm' # actually in cm in file! convert further down!\n",
    "    elif vari in ['wind_speed']:\n",
    "        long_name   = '10m wind speed'\n",
    "        unit        = 'm s-1'\n",
    "    elif vari in ['diat_Fe_lim_surf']:\n",
    "        long_name   = 'diatom surface iron limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diat_N_lim_surf']:\n",
    "        long_name   = 'diatom surface nitrate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diat_P_lim_surf']:\n",
    "        long_name   = 'diatom surface phosphate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diat_SiO3_lim_surf']:\n",
    "        long_name   = 'diatom surface silicate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['sp_Fe_lim_surf']:\n",
    "        long_name   = 'SP surface iron limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['sp_N_lim_surf']:\n",
    "        long_name   = 'SP surface nitrate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['sp_P_lim_surf']:\n",
    "        long_name   = 'SP surface phosphate limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diatChl_SURF']:\n",
    "        long_name   = 'diatom surface chlorophyll'\n",
    "        unit        = 'mg chl m-3'\n",
    "    elif vari in ['spChl_SURF']:\n",
    "        long_name   = 'SP surface chlorophyll'\n",
    "        unit        = 'mg chl m-3'\n",
    "    elif vari in ['diat_light_lim_surf']:\n",
    "        long_name   = 'diatom surface light limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['sp_light_lim_surf']:\n",
    "        long_name   = 'SP surface light limitation'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['diat_specific_growth_rate_surf','mu_diat']:\n",
    "        long_name   = 'diatom surface specific growth rate'\n",
    "        unit        = 'd-1'\n",
    "    elif vari in ['sp_specific_growth_rate_surf','mu_sp']:\n",
    "        long_name   = 'SP surface specific growth rate'\n",
    "        unit        = 'd-1'\n",
    "    elif vari in ['phytoC_zint_100m']:\n",
    "        long_name   = 'total integrated phytoplankton C biomass in the top 100m'\n",
    "        unit        = 'mmol C m-3'\n",
    "    elif vari in ['photoC_zint']:\n",
    "        long_name   = 'total vertically integrated NPP'\n",
    "        unit        = 'mmol m-3 cm s-1 m2'\n",
    "    elif vari in ['cloudfrac_isccp']:\n",
    "        long_name   = 'ISCCP cloud cover'\n",
    "        unit        = 'n.d.'\n",
    "    elif vari in ['PAR_incoming']:\n",
    "        long_name   = 'Incoming PAR (45% of incoming shortwave radiation)'\n",
    "        unit        = 'W m-2'\n",
    "    elif vari in ['slp']:\n",
    "        long_name   = 'sea level pressure'\n",
    "        unit        = 'Pa'\n",
    "   \n",
    "    if vari_list[vv] in ['totChl_hr']:\n",
    "        path1 = '/global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/'+vari+'_hr_anomalies/'\n",
    "    else:\n",
    "        path1 = '/global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/'+vari+'_anomalies/'\n",
    "        \n",
    "    # where to save anomaly files?\n",
    "    savepath     = path1 #+'TEST/' #'/global/cfs/cdirs/m4003/cnissen/CESM_'+vari+'_anomalies/'\n",
    "    # check existence of paths\n",
    "    if not os.path.exists(savepath):\n",
    "        print ('Created '+savepath)\n",
    "        os.makedirs(savepath)\n",
    "        \n",
    "    if vari_list[vv]=='totChl_hr':\n",
    "        ds = xr.open_mfdataset(path1+'Anomalies_within_1000km_of_storm_center_'+vari+'_hr_JRA_grid_*noon'+\\\n",
    "                               time_string+'_subtract_clim_first.nc',\\\n",
    "                           concat_dim='count_anom',combine='nested')\n",
    "    elif vari_list[vv]=='totChl_emulator':\n",
    "        ds = xr.open_mfdataset(path1+'Anomalies_within_1000km_of_storm_center_'+vari+'_JRA_grid_*noon'+\\\n",
    "                               time_string+'_subtract_clim_first_gap_filled_clim.nc',\\\n",
    "                           concat_dim='count_anom',combine='nested')\n",
    "        #ds = xr.open_mfdataset(path1+'Anomalies_within_1000km_of_storm_center_'+vari+'_JRA_grid_*noon'+\\\n",
    "        #                       time_string+'_subtract_clim_first_full_field_clim.nc',\\\n",
    "        #                   concat_dim='count_anom',combine='nested')\n",
    "    else:\n",
    "        ds = xr.open_mfdataset(path1+'Anomalies_within_1000km_of_storm_center_'+vari+'_JRA_grid_*noon'+time_string+\\\n",
    "                               '_subtract_clim_first.nc',\\\n",
    "                           concat_dim='count_anom',combine='nested')\n",
    "    print(ds['index_storm'])\n",
    "    print()\n",
    "\n",
    "    res    = 100\n",
    "    x_bins = np.arange(-1000,1000+res,res)\n",
    "    y_bins = np.arange(-1000,1000+res,res)\n",
    "\n",
    "    dist_threshold = 1000\n",
    "\n",
    "    # load lat/lon  \n",
    "    if vari_list[vv] in ['totChl_hr']:\n",
    "        file1 = 'Anomalies_within_1000km_of_storm_center_'+vari+'_hr_JRA_grid_1997-01-01_all_at_noon'+\\\n",
    "                        time_string+'_subtract_clim_first.nc'\n",
    "    elif vari_list[vv] in ['totChl_emulator']:\n",
    "        file1 = 'Anomalies_within_1000km_of_storm_center_'+vari+'_JRA_grid_1997-01-01_all_at_noon'+\\\n",
    "                        time_string+'_subtract_clim_first_gap_filled_clim.nc'\n",
    "        #file1 = 'Anomalies_within_1000km_of_storm_center_'+vari+'_JRA_grid_1997-01-01_all_at_noon'+\\\n",
    "        #                time_string+'_subtract_clim_first_full_field_clim.nc'\n",
    "    else:\n",
    "        file1 = 'Anomalies_within_1000km_of_storm_center_'+vari+'_JRA_grid_1997-01-01_all_at_noon'+\\\n",
    "                        time_string+'_subtract_clim_first.nc'\n",
    "    ff2  = xr.open_dataset(path1+file1)\n",
    "    lat       = ff2['lat'].values #[0:150] # model grid\n",
    "    lon       = ff2['lon'].values # model grid\n",
    "    lat,lon = np.meshgrid(lat,lon)\n",
    "    lat = lat.transpose()\n",
    "    lon = lon.transpose()\n",
    "    ff2.close()\n",
    "\n",
    "    #index_storm = np.asarray([int(x) for x in ds['index_storm']])\n",
    "    list_storms = np.unique(ds['index_storm']) #np.asarray([int(x) for x in np.unique(ds['index_storm'])])\n",
    "    num_storms = len(np.unique(ds['index_storm']))\n",
    "    print('num_storms (ALL YEARS):',num_storms)\n",
    "\n",
    "    #---\n",
    "    # create netcdf file\n",
    "    #---\n",
    "    if save_netcdf:\n",
    "        fv = -999\n",
    "        if vari_list[vv] in ['totChl_hr']:\n",
    "            netcdf_name = 'Composite_abs_field_within_'+str(dist_threshold)+'km_of_storm_center_at_noon_'+\\\n",
    "                                vari+'_hr_'+str(years[0])+'_'+str(years[-1])+time_string+'_subtract_clim_first.nc'\n",
    "        else:\n",
    "            netcdf_name = 'Composite_abs_field_within_'+str(dist_threshold)+'km_of_storm_center_at_noon_'+\\\n",
    "                                vari+'_'+str(years[0])+'_'+str(years[-1])+time_string+'_subtract_clim_first_gap_filled_clim.nc'\n",
    "            #netcdf_name = 'Composite_abs_field_within_'+str(dist_threshold)+'km_of_storm_center_at_noon_'+\\\n",
    "            #                    vari+'_'+str(years[0])+'_'+str(years[-1])+time_string+'_subtract_clim_first.nc'\n",
    "        if not os.path.exists(savepath+netcdf_name):\n",
    "            print('Create file '+savepath+netcdf_name)\n",
    "            w_nc_fid = Dataset(savepath+netcdf_name, 'w', format='NETCDF4_CLASSIC')\n",
    "            w_nc_fid.contact     = 'Cara Nissen, cara.nissen@colorado.edu'\n",
    "            w_nc_fid.source_data = path1+file1\n",
    "            w_nc_fid.script      = '/global/homes/c/cnissen/scripts/save_CESM_daily_chl_fields_composites_v3_subtract_clim_first_CORRECTED.ipynb'\n",
    "            w_nc_fid.sea_ice     = 'sea ice area is masked and not considered in composites'\n",
    "            # create dimension & variable\n",
    "            w_nc_fid.createDimension('x_bins', len(x_bins)) \n",
    "            w_nc_fid.createDimension('y_bins', len(y_bins)) \n",
    "            w_nc_fid.createDimension('num_storms', num_storms) \n",
    "            #w_nc_fid.createDimension('time', time_all[365:,:,:].shape[0]) \n",
    "\n",
    "            w_nc_var1 = w_nc_fid.createVariable(vari+'_storm_mean', 'f4',('num_storms','y_bins','x_bins'),fill_value=fv)\n",
    "            w_nc_var1.long_name = 'binned mean '+long_name+' anomaly as a function of the distance to the storm center'\n",
    "            w_nc_var1.units = unit\n",
    "                \n",
    "            if vari in ['totChl','totChl_emulator','FG_CO2_2','photoC_total_surf','diatChl_SURF','spChl_SURF']:\n",
    "                w_nc_var1 = w_nc_fid.createVariable(vari+'_storm_integral', 'f4',('num_storms','y_bins','x_bins'),fill_value=fv)\n",
    "                w_nc_var1.long_name = 'binned integrated '+long_name+' anomaly as a function of the distance to the storm center'\n",
    "                w_nc_var1.note = 'integral for each storm over each anomaly at noon'\n",
    "                w_nc_var1.units = unit+' * num_days for which storm exists'\n",
    "                    \n",
    "            w_nc_var1 = w_nc_fid.createVariable('x_bins', 'f4',('x_bins'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Bins in x-direction (lon); '\n",
    "            w_nc_var1 = w_nc_fid.createVariable('y_bins', 'f4',('y_bins'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Bins in y-direction (lat); '\n",
    "\n",
    "            w_nc_var1 = w_nc_fid.createVariable('lon_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Longitude of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('lat_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Latitude of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('year_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Year of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('month_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Month of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('day_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Day of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('hour_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Hour of storm center at min. SLP'\n",
    "\n",
    "            w_nc_var1 = w_nc_fid.createVariable('days_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Number of days for which storm exists'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('days_storm_with_data','f4',('num_storms','x_bins','y_bins'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Number of days of storm existence for which data exists'\n",
    "                \n",
    "            w_nc_var1 = w_nc_fid.createVariable('avg_min_slp_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Average min. sea level pressure for each storm (averaged over each min. SLP at noon)'\n",
    "            w_nc_var1.units = 'Pa'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('min_min_slp_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Minimum sea level pressure during the existence of each storm (based on min. SLP at noon)'\n",
    "            w_nc_var1.units = 'Pa'\n",
    "                \n",
    "            # write lat/lon to file\n",
    "            w_nc_fid.variables['x_bins'][:]  = x_bins\n",
    "            w_nc_fid.variables['y_bins'][:]  = y_bins\n",
    "\n",
    "            w_nc_fid.close()\n",
    "\n",
    "    # loop over storms\n",
    "    counter = 0\n",
    "    for ss in tqdm(list_storms): #range(0,num_storms)):\n",
    "\n",
    "        ind_storm  = np.where(ds['index_storm']==ss)[0] \n",
    "        #print(ind_storm.shape[0])\n",
    "        aux_array = ds['min_slp_storm'][ind_storm].values\n",
    "\n",
    "        # additional information to be stored in file\n",
    "        days_storm = ind_storm.shape[0] # duration of the storm -> to be stored in the file\n",
    "        min_slp_avg_storm = np.mean(ds['min_slp_storm'][ind_storm]) # avg min. SLP for all considered time steps -> to be stored in the file\n",
    "        min_slp_min_storm = np.min(ds['min_slp_storm'][ind_storm]) # minimum min. SLP for all considered time steps -> to be stored in the fil\n",
    "        ind_min = np.where(aux_array==np.nanmin(aux_array))[0][0]\n",
    "        aux_lat2  = ds['lat_storm'][ind_storm][ind_min]\n",
    "        aux_lon2  = ds['lon_storm'][ind_storm][ind_min]\n",
    "        aux_year  = ds['year_storm'][ind_storm][ind_min]\n",
    "        aux_month = ds['month_storm'][ind_storm][ind_min]\n",
    "        aux_day   = ds['day_storm'][ind_storm][ind_min]\n",
    "        del aux_array,ind_min\n",
    "\n",
    "        data_storm_mean2  = np.nan*np.ones([ind_storm.shape[0],len(x_bins),len(y_bins)]) # for each 100kmx100km bin, calculate the mean \n",
    "        counter_existing_days = np.zeros([len(x_bins),len(y_bins)]) #0 # count the number of days for which data exist (relevant for ISCCP!)\n",
    "        for ii in range(0,ind_storm.shape[0]): # loop over each day the storm exists\n",
    "            if vari in ['totChl_emulator']:\n",
    "                aux_data_anom2 = ds['totChl_storm'][ind_storm[ii],:,:].values.ravel()\n",
    "            elif vari in ['MLD_hr']:\n",
    "                aux_data_anom2 = ds['HMXL_2_storm'][ind_storm[ii],:,:].values.ravel()\n",
    "            else:\n",
    "                aux_data_anom2 = ds[vari+'_storm'][ind_storm[ii],:,:].values.ravel()\n",
    "\n",
    "            # mask all cells that contain sea ice (analyze separately)\n",
    "            aux_ice = ds['sea_ice'][ind_storm[ii],:,:].values.ravel()\n",
    "            aux_data_anom2[aux_ice>0] = np.nan\n",
    "\n",
    "            lat2 = np.copy(lat).ravel()\n",
    "            lon2 = np.copy(lon).ravel()\n",
    "            lat2 = np.delete(lat2,np.isnan(aux_data_anom2))\n",
    "            lon2 = np.delete(lon2,np.isnan(aux_data_anom2))\n",
    "            aux_data_anom2 = np.delete(aux_data_anom2,np.isnan(aux_data_anom2))\n",
    "\n",
    "            # position of current storm \n",
    "            aux_lat = ds['lat_storm'].values[ind_storm[ii]]\n",
    "            aux_lon = ds['lon_storm'].values[ind_storm[ii]]\n",
    "            point_storm = (aux_lat,aux_lon)\n",
    "\n",
    "            # get distance to storm center of each available point \n",
    "            points_distance_x2,points_distance_y2,points_data2 = get_distance_to_storm_center(lat2,lon2,aux_lat,aux_lon)\n",
    "\n",
    "            # bin all points as a function of the distance to the storm center\n",
    "            data_storm_mean2[ii,:,:] = bin_points_as_distance_to_storm_center(counter,points_distance_x2,points_distance_y2,\\\n",
    "                                                                                x_bins,y_bins,aux_data_anom2,\\\n",
    "                                                                            data_storm_mean2[ii,:,:])\n",
    "            del points_distance_x2,points_distance_y2,points_data2\n",
    "            del aux_data_anom2,aux_lat,aux_lon,point_storm,lat2,lon2\n",
    "            # NEW VERSION (Sep 23, 2024): store the info of exisitng time steps for each location in the vicinity of the storm!\n",
    "            #   (if I only store one number for whether or not data exist anywhere near a storm, \n",
    "            #   the weighting of composites with the storm length will be off for some pixels)\n",
    "            for mm in range(0,len(x_bins)):\n",
    "                for nn in range(0,len(y_bins)):\n",
    "                    if ~np.isnan(data_storm_mean2[ii,mm,nn]):\n",
    "                        counter_existing_days[mm,nn] = counter_existing_days[mm,nn]+1\n",
    "\n",
    "        #----\n",
    "        # get avg/integrated anomaly for current storm\n",
    "        #----\n",
    "        # only get integrals for some of the variables\n",
    "        if vari in ['totChl','totChl_emulator','FG_CO2_2','photoC_total_surf','diatChl_SURF','spChl_SURF']:\n",
    "            data_storm_mean2_int  = np.nansum(data_storm_mean2,axis=0)\n",
    "\n",
    "        # anomaly 2\n",
    "        data_storm_mean2  = np.nanmean(data_storm_mean2,axis=0)\n",
    "    \n",
    "        if save_netcdf:\n",
    "            if vari in ['totChl','totChl_emulator','FG_CO2_2','photoC_total_surf','diatChl_SURF','spChl_SURF']:\n",
    "                data_storm_mean2_int[np.isnan(data_storm_mean2_int)]   = fv\n",
    "                data_storm_mean2_int[data_storm_mean2_int==0]   = fv\n",
    "                    \n",
    "            data_storm_mean2[np.isnan(data_storm_mean2)]   = fv\n",
    "                \n",
    "            w_nc_fid = Dataset(savepath+netcdf_name, 'r+', format='NETCDF4_CLASSIC') \n",
    "            if vari in ['totChl','totChl_emulator','FG_CO2_2','photoC_total_surf','diatChl_SURF','spChl_SURF']:\n",
    "                w_nc_fid.variables[vari+'_storm_integral'][counter,:,:]  = data_storm_mean2_int\n",
    "                    \n",
    "            w_nc_fid.variables[vari+'_storm_mean'][counter,:,:]  = data_storm_mean2\n",
    "                \n",
    "            w_nc_fid.variables['lat_storm'][counter]   = aux_lat2\n",
    "            w_nc_fid.variables['lon_storm'][counter]   = aux_lon2\n",
    "            w_nc_fid.variables['year_storm'][counter]  = aux_year\n",
    "            w_nc_fid.variables['month_storm'][counter] = aux_month\n",
    "            w_nc_fid.variables['day_storm'][counter]   = aux_day\n",
    "            w_nc_fid.variables['hour_storm'][counter]  = 12\n",
    "                \n",
    "            w_nc_fid.variables['days_storm'][counter]        = days_storm\n",
    "            w_nc_fid.variables['days_storm_with_data'][counter,:,:] = counter_existing_days # NEW VERSION (store 2D info for each storm)\n",
    "            w_nc_fid.variables['avg_min_slp_storm'][counter] = min_slp_avg_storm\n",
    "            w_nc_fid.variables['min_min_slp_storm'][counter] = min_slp_min_storm\n",
    "                \n",
    "            w_nc_fid.close()  \n",
    "        \n",
    "        counter = counter+1\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a09d5b-2c7a-48a8-aa88-da4d71174518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c02ad2-4b0d-4f8f-a925-57b4d96b4a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36fe0a4-a6e4-4a96-a9c3-95457f48169c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-jupyter",
   "language": "python",
   "name": "myenv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
