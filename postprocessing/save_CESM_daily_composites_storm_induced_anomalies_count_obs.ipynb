{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5fbc2e-f891-4261-9e91-23e0cee673d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data availability for the different model data sets \n",
    "# calculate once and store as a netcdf file\n",
    "# basis for Fig. 2 in paper\n",
    "#\n",
    "# REVISE HOW I STORE THE \"COUNT\" THAT GOES INTO CALCULATION OF ANOMALY\n",
    "# here: for every day a storm exists, create a mask with ones where there is data\n",
    "#       then, sum the invidual masks up over the whole existence of a storm\n",
    "#       -> point that have data at every time step will have the number of days as the number of obs\n",
    "#       Note that there can be time steps of the storm's existence with no data if the storm is entirely over land/ice\n",
    "#\n",
    "# only need to run this once for each year, as the data coverage is the same for all variables\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7d5f16-a930-4b60-bbf5-45c0cc4c83cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset, MFDataset\n",
    "import numba as nb\n",
    "import time as timing\n",
    "from numba import njit \n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "from geopy.distance import distance\n",
    "import seawater as sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "656a1595-5147-4f7c-968a-a52201627053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---\n",
    "# FUNCTION \n",
    "#---\n",
    "\n",
    "def get_distance_to_storm_center(lat2,lon2,aux_lat,aux_lon):\n",
    "                \n",
    "    # create list of locations within 1000km of the storm\n",
    "    points_data = []\n",
    "    for pp in range(0,lat2.shape[0]):\n",
    "        aux = (lat2[pp],lon2[pp])\n",
    "        points_data.append(aux)\n",
    "        del aux\n",
    "\n",
    "    #print(len(points_data))\n",
    "    # for each of these points get the distance to the storm center in km -> get distance in x-dir and y-dir\n",
    "    points_distance_x = np.zeros(len(points_data)) # distance in longitudinal direction, i.e., use latitude of storm (aux_lat)\n",
    "    points_distance_y = np.zeros(len(points_data)) # distance in latitudinal direction, i.e., use longitude of storm (aux_lon)\n",
    "    for pp in range(0,len(points_data)): \n",
    "        # distance in longitudinal direction\n",
    "        aux_point = (aux_lat,points_data[pp][1])\n",
    "        points_distance_x[pp] = distance(point_storm, aux_point).km\n",
    "       # print(aux_point,point_storm,points_distance_x[pp])\n",
    "        # check sign: if lon grid cell is smaller (=further west) than lon of storm, define distance to be negative\n",
    "        if points_data[pp][1]<aux_lon:\n",
    "            points_distance_x[pp] = -1*points_distance_x[pp]\n",
    "        elif (aux_lon<0) & (points_data[pp][1]>0): # lon_storm is east of dateline, but grid cell is west of dateline (grid cell is also further west in this case!)\n",
    "            if (points_data[pp][1]-360)<aux_lon:\n",
    "                points_distance_x[pp] = -1*points_distance_x[pp]\n",
    "        del aux_point\n",
    "        # distance in latitudinal direction\n",
    "        aux_point = (points_data[pp][0],aux_lon)\n",
    "        points_distance_y[pp] = distance(point_storm, aux_point).km\n",
    "        # check sign: if lat grid cell is smaller (=further south) than lat of storm, define distance to be negative\n",
    "        if points_data[pp][0]<aux_lat:\n",
    "            points_distance_y[pp] = -1*points_distance_y[pp]\n",
    "        del aux_point  \n",
    "    return points_distance_x,points_distance_y,points_data\n",
    "                    \n",
    "    \n",
    "def bin_points_as_distance_to_storm_center(counter,points_distance_x,points_distance_y,x_bins,y_bins,aux_data_anom,data_storm_mean,data_storm_std,data_storm_count):\n",
    "    # data_storm_mean,data_storm_std,data_storm_count: initialized arrays, will be filled in this function and then returned\n",
    "    \n",
    "    # bin the points (account for where each point is relative to storm center)\n",
    "    ind_x = np.digitize(points_distance_x,x_bins,right=False) # minimum is 1 (not zero!!)\n",
    "    ind_y = np.digitize(points_distance_y,y_bins,right=False)\n",
    " #   print(np.min(ind_x),np.max(ind_x))\n",
    " #   print(np.min(ind_y),np.max(ind_y))\n",
    "    # returned index satisfies: bins[i-1] <= x < bins[i]\n",
    "\n",
    "   # print(aux_data_anom.shape)\n",
    "    for xx in range(1,len(x_bins)+1): # start at 1 here -> see note above for ind_x\n",
    "        for yy in range(1,len(x_bins)+1):\n",
    "            index = np.where((ind_y==yy) & (ind_x==xx))[0]\n",
    "            if len(index)>0:\n",
    "                #if counter==6: \n",
    "                #    print(xx,yy,index.shape,aux_data_anom.shape)\n",
    "                #if (counter==40) & (xx==11) & (yy==20):\n",
    "                #    print(xx,yy,len(points_distance_x))\n",
    "                #    print(index)\n",
    "                #    print(index.shape,aux_data_anom.shape)\n",
    "                # anomaly 2\n",
    "                data_storm_mean[xx-1,yy-1]  = np.nanmean(aux_data_anom[index])\n",
    "                data_storm_std[xx-1,yy-1]   = np.nanstd(aux_data_anom[index])\n",
    "                data_storm_count[xx-1,yy-1] = index.shape[0]\n",
    "            del index\n",
    "    return data_storm_mean,data_storm_std,data_storm_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e6c2ace-5764-4186-b4ad-5a3a8fef0b18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process  totChl \n",
      "<xarray.DataArray 'index_storm' (count_anom: 44258)>\n",
      "dask.array<concatenate, shape=(44258,), dtype=float32, chunksize=(2126,), chunktype=numpy.ndarray>\n",
      "Dimensions without coordinates: count_anom\n",
      "Attributes:\n",
      "    description:  index of storm (to know which storm imprints belong to the ...\n",
      "\n",
      "num_storms (ALL YEARS): 9615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9615/9615 [3:25:40<00:00,  1.28s/it]  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lon_storm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m data_storm_mean2,data_storm_std2,data_storm_count2\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m days_storm,min_slp_avg_storm,min_slp_min_storm,aux_lat2,aux_lon2,aux_year,aux_month,aux_day\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m lon_storm,lat_storm,year_storm,month_storm,day_storm,hour_storm,lat,lon\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lon_storm' is not defined"
     ]
    }
   ],
   "source": [
    "#---\n",
    "# LOAD INFO FROM ALL YEARS AT ONCE\n",
    "#----\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', message='Mean of empty slice')\n",
    "\n",
    "years = np.arange(1997,2018+1,1) \n",
    "\n",
    "save_netcdf = True\n",
    "\n",
    "vari_list = ['totChl','totChl_emulator','totChl_hr'] \n",
    "time_string = ''\n",
    "\n",
    "for vv in range(0,1):#len(vari_list)):\n",
    "    \n",
    "    vari = vari_list[vv] \n",
    "    print('Process ',vari,time_string)    \n",
    "\n",
    "    if vari in ['totChl']:\n",
    "        long_name   = 'total chlorophyll'\n",
    "        unit        = 'mg chl m-3'\n",
    "    \n",
    "    path1 = '/global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/'+vari+'_anomalies/'\n",
    "\n",
    "    # where to save anomaly files?\n",
    "    savepath     = '/global/cfs/cdirs/m4003/cnissen/CESM_anomalies_STORM_PAPER_subtract_clim_first/count_obs_in_anomalies/'\n",
    "    # check existence of paths\n",
    "    if not os.path.exists(savepath):\n",
    "        print ('Created '+savepath)\n",
    "        os.makedirs(savepath)\n",
    "        \n",
    "    if vari_list[vv]=='totChl_hr':\n",
    "        ds = xr.open_mfdataset(path1+'Anomalies_within_1000km_of_storm_center_'+vari+'_JRA_grid_*noon'+\\\n",
    "                               time_string+'_subtract_clim_first.nc',\\\n",
    "                           concat_dim='count_anom',combine='nested')\n",
    "    else:\n",
    "        ds = xr.open_mfdataset(path1+'Anomalies_within_1000km_of_storm_center_'+vari+'_JRA_grid_*noon'+time_string+\\\n",
    "                               '_subtract_clim_first.nc',\\\n",
    "                           concat_dim='count_anom',combine='nested')\n",
    "    print(ds['index_storm'])\n",
    "    print()\n",
    "\n",
    "    res    = 100\n",
    "    x_bins = np.arange(-1000,1000+res,res)\n",
    "    y_bins = np.arange(-1000,1000+res,res)\n",
    "\n",
    "    dist_threshold = 1000\n",
    "\n",
    "    # load lat/lon  \n",
    "    if vari_list[vv] in ['totChl_hr']:\n",
    "        file1 = 'Anomalies_within_1000km_of_storm_center_'+vari+'_JRA_grid_1997-01-01_all_at_noon'+\\\n",
    "                        time_string+'_subtract_clim_first.nc'\n",
    "    elif vari_list[vv] in ['totChl_emulator']:\n",
    "        file1 = 'Anomalies_within_1000km_of_storm_center_'+vari+'_JRA_grid_1997-01-01_all_at_noon'+\\\n",
    "                        time_string+'_subtract_clim_first_full_field_clim.nc'\n",
    "    else:\n",
    "        file1 = 'Anomalies_within_1000km_of_storm_center_'+vari+'_JRA_grid_1997-01-01_all_at_noon'+\\\n",
    "                        time_string+'_subtract_clim_first.nc'\n",
    "    ff2  = xr.open_dataset(path1+file1)\n",
    "    lat       = ff2['lat'].values #[0:150] # model grid\n",
    "    lon       = ff2['lon'].values # model grid\n",
    "    lat,lon = np.meshgrid(lat,lon)\n",
    "    lat = lat.transpose()\n",
    "    lon = lon.transpose()\n",
    "    ff2.close()\n",
    "\n",
    "    #index_storm = np.asarray([int(x) for x in ds['index_storm']])\n",
    "    list_storms = np.unique(ds['index_storm']) #np.asarray([int(x) for x in np.unique(ds['index_storm'])])\n",
    "    num_storms = len(np.unique(ds['index_storm']))\n",
    "    print('num_storms (ALL YEARS):',num_storms)\n",
    "\n",
    "    #---\n",
    "    # create netcdf file\n",
    "    #---\n",
    "    if save_netcdf:\n",
    "        fv = -999\n",
    "            \n",
    "        if vari=='totChl_emulator':\n",
    "            netcdf_name = 'Composite_anomalies_within_'+str(dist_threshold)+'km_of_storm_center_at_noon'+\\\n",
    "                                    '_emulator_'+str(years[0])+'_'+str(years[-1])+time_string+'_subtract_clim_first_full_field_clim_COUNT_OBS.nc'\n",
    "        elif vari=='totChl_hr':\n",
    "            netcdf_name = 'Composite_anomalies_within_'+str(dist_threshold)+'km_of_storm_center_at_noon'+\\\n",
    "                                    '_hr_'+str(years[0])+'_'+str(years[-1])+time_string+'_subtract_clim_first_COUNT_OBS.nc'\n",
    "        else:\n",
    "            netcdf_name = 'Composite_anomalies_within_'+str(dist_threshold)+'km_of_storm_center_at_noon'+\\\n",
    "                                    '_'+str(years[0])+'_'+str(years[-1])+time_string+'_subtract_clim_first_COUNT_OBS.nc'\n",
    "        if not os.path.exists(savepath+netcdf_name):\n",
    "            print('Create file '+savepath+netcdf_name)\n",
    "            w_nc_fid = Dataset(savepath+netcdf_name, 'w', format='NETCDF4_CLASSIC')\n",
    "            w_nc_fid.contact     = 'Cara Nissen, cara.nissen@colorado.edu'\n",
    "            w_nc_fid.source_data = path1+file1\n",
    "            w_nc_fid.script      = '/global/homes/c/cnissen/scripts/save_CESM_daily_chl_anomalies_composites_v2_count_obs.ipynb'\n",
    "            w_nc_fid.sea_ice     = 'sea ice area is masked and not considered in composites'\n",
    "            # create dimension & variable\n",
    "            w_nc_fid.createDimension('x_bins', len(x_bins)) \n",
    "            w_nc_fid.createDimension('y_bins', len(y_bins)) \n",
    "            w_nc_fid.createDimension('num_storms', num_storms) \n",
    "            #w_nc_fid.createDimension('time', time_all[365:,:,:].shape[0]) \n",
    "\n",
    "            w_nc_var1 = w_nc_fid.createVariable(vari+'_storm_anomaly4_count', 'f4',('num_storms','y_bins','x_bins'),fill_value=fv)\n",
    "            w_nc_var1.long_name = 'number of grid cells used in averaging'\n",
    "            w_nc_var1.step1 = '1) for each day the storm exists, create a mask where data is available' \n",
    "            w_nc_var1.step2 = '2) sum over all time steps' \n",
    "                \n",
    "            w_nc_var1 = w_nc_fid.createVariable(vari+'_storm_anomaly4_count_max', 'f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.long_name = 'days that each storm exists'\n",
    "            w_nc_var1.note = 'some days might not have data if storm is entirely over land or sea ice' \n",
    "                \n",
    "            w_nc_var1 = w_nc_fid.createVariable('x_bins', 'f4',('x_bins'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Bins in x-direction (lon); '\n",
    "            w_nc_var1 = w_nc_fid.createVariable('y_bins', 'f4',('y_bins'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Bins in y-direction (lat); '\n",
    "\n",
    "            w_nc_var1 = w_nc_fid.createVariable('lon_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Longitude of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('lat_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Latitude of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('year_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Year of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('month_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Month of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('day_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Day of storm center at min. SLP'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('hour_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Hour of storm center at min. SLP'\n",
    "\n",
    "            w_nc_var1 = w_nc_fid.createVariable('days_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Number of days for which storm exists'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('avg_min_slp_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Average min. sea level pressure for each storm (averaged over each min. SLP at noon)'\n",
    "            w_nc_var1.units = 'Pa'\n",
    "            w_nc_var1 = w_nc_fid.createVariable('min_min_slp_storm','f4',('num_storms'),fill_value=fv)\n",
    "            w_nc_var1.description = 'Minimum sea level pressure during the existence of each storm (based on min. SLP at noon)'\n",
    "            w_nc_var1.units = 'Pa'\n",
    "                \n",
    "            # write lat/lon to file\n",
    "            w_nc_fid.variables['x_bins'][:]  = x_bins\n",
    "            w_nc_fid.variables['y_bins'][:]  = y_bins\n",
    "\n",
    "            w_nc_fid.close()\n",
    "            \n",
    "    # loop over storms\n",
    "    counter = 0\n",
    "    for ss in tqdm(list_storms): #range(0,num_storms)):\n",
    "\n",
    "        ind_storm  = np.where(ds['index_storm']==ss)[0] \n",
    "        #print(ind_storm.shape[0])\n",
    "        aux_array = ds['min_slp_storm'][ind_storm].values\n",
    "\n",
    "        # additional information to be stored in file\n",
    "        days_storm = ind_storm.shape[0] # duration of the storm -> to be stored in the file\n",
    "        min_slp_avg_storm = np.mean(ds['min_slp_storm'][ind_storm]) # avg min. SLP for all considered time steps -> to be stored in the file\n",
    "        min_slp_min_storm = np.min(ds['min_slp_storm'][ind_storm]) # minimum min. SLP for all considered time steps -> to be stored in the fil\n",
    "        ind_min = np.where(aux_array==np.nanmin(aux_array))[0][0]\n",
    "        aux_lat2  = ds['lat_storm'][ind_storm][ind_min]\n",
    "        aux_lon2  = ds['lon_storm'][ind_storm][ind_min]\n",
    "        aux_year  = ds['year_storm'][ind_storm][ind_min]\n",
    "        aux_month = ds['month_storm'][ind_storm][ind_min]\n",
    "        aux_day   = ds['day_storm'][ind_storm][ind_min]\n",
    "        del aux_array,ind_min\n",
    "\n",
    "        data_storm_mean2  = np.nan*np.ones([ind_storm.shape[0],len(x_bins),len(y_bins)]) # for each 100kmx100km bin, calculate the mean \n",
    "        data_storm_std2   = np.nan*np.ones([ind_storm.shape[0],len(x_bins),len(y_bins)]) # for each 100kmx100km bin, calculate the std \n",
    "        data_storm_count2 = np.nan*np.ones([ind_storm.shape[0],len(x_bins),len(y_bins)]) # count how many grid cells were used in averaging\n",
    "        for ii in range(0,ind_storm.shape[0]): # loop over each day the storm exists\n",
    "            if vari in ['totChl_emulator','totChl_hr']:\n",
    "                aux_data_anom2 = ds['totChl_storm_anomaly4'][ind_storm[ii],:,:].values.ravel()\n",
    "            else:\n",
    "                aux_data_anom2 = ds[vari+'_storm_anomaly4'][ind_storm[ii],:,:].values.ravel()\n",
    "\n",
    "            # mask all cells that contain sea ice (analyze separately)\n",
    "            aux_ice = ds['sea_ice'][ind_storm[ii],:,:].values.ravel()\n",
    "            aux_data_anom2[aux_ice>0] = np.nan\n",
    "\n",
    "            lat2 = np.copy(lat).ravel()\n",
    "            lon2 = np.copy(lon).ravel()\n",
    "            lat2 = np.delete(lat2,np.isnan(aux_data_anom2))\n",
    "            lon2 = np.delete(lon2,np.isnan(aux_data_anom2))\n",
    "            aux_data_anom2 = np.delete(aux_data_anom2,np.isnan(aux_data_anom2))\n",
    "\n",
    "            # position of current storm \n",
    "            aux_lat = ds['lat_storm'].values[ind_storm[ii]]\n",
    "            aux_lon = ds['lon_storm'].values[ind_storm[ii]]\n",
    "            point_storm = (aux_lat,aux_lon)\n",
    "\n",
    "            # get distance to storm center of each available point \n",
    "            points_distance_x2,points_distance_y2,points_data2 = get_distance_to_storm_center(lat2,lon2,aux_lat,aux_lon)\n",
    "\n",
    "            # bin all points as a function of the distance to the storm center\n",
    "            data_storm_mean2[ii,:,:],data_storm_std2[ii,:,:],data_storm_count2[ii,:,:] = bin_points_as_distance_to_storm_center(counter,points_distance_x2,points_distance_y2,\\\n",
    "                                                                                                                                        x_bins,y_bins,aux_data_anom2,\\\n",
    "                                                                                                        data_storm_mean2[ii,:,:],data_storm_std2[ii,:,:],data_storm_count2[ii,:,:])\n",
    "            del points_distance_x2,points_distance_y2,points_data2\n",
    "            del aux_data_anom2,aux_lat,aux_lon,point_storm,lat2,lon2\n",
    "                \n",
    "            # TEST -> only count which grid cell is covered\n",
    "            data_storm_count2[ii,:,:][data_storm_count2[ii,:,:]>0] = 1   \n",
    "                \n",
    "        # anomaly 7\n",
    "        data_storm_count2 = np.nansum(data_storm_count2,axis=0) # TEST nansum instead of nanmean -> store this!\n",
    "\n",
    "        test_plot = False\n",
    "        if test_plot:\n",
    "            print('sum over all time steps...')\n",
    "            fig = plt.figure(figsize=(6,5))\n",
    "            plt.pcolor(data_storm_count2,vmin=0,vmax=ind_storm.shape[0],cmap=plt.cm.RdYlBu_r)\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "                \n",
    "        if save_netcdf:\n",
    "            data_storm_count2[np.isnan(data_storm_count2)] = fv\n",
    "            data_storm_count2[data_storm_count2==0] = fv\n",
    "                \n",
    "            w_nc_fid = Dataset(savepath+netcdf_name, 'r+', format='NETCDF4_CLASSIC') \n",
    "                    \n",
    "            w_nc_fid.variables[vari+'_storm_anomaly4_count'][counter,:,:] = data_storm_count2\n",
    "            w_nc_fid.variables[vari+'_storm_anomaly4_count_max'][counter] = ind_storm.shape[0]\n",
    "                \n",
    "            w_nc_fid.variables['lat_storm'][counter]   = aux_lat2\n",
    "            w_nc_fid.variables['lon_storm'][counter]   = aux_lon2\n",
    "            w_nc_fid.variables['year_storm'][counter]  = aux_year\n",
    "            w_nc_fid.variables['month_storm'][counter] = aux_month\n",
    "            w_nc_fid.variables['day_storm'][counter]   = aux_day\n",
    "            w_nc_fid.variables['hour_storm'][counter]  = 12\n",
    "                \n",
    "            w_nc_fid.variables['days_storm'][counter]        = days_storm\n",
    "            w_nc_fid.variables['avg_min_slp_storm'][counter] = min_slp_avg_storm\n",
    "            w_nc_fid.variables['min_min_slp_storm'][counter] = min_slp_min_storm\n",
    "                \n",
    "            w_nc_fid.close()  \n",
    "                \n",
    "        counter = counter+1\n",
    "\n",
    "        del data_storm_mean2,data_storm_std2,data_storm_count2\n",
    "        del days_storm,min_slp_avg_storm,min_slp_min_storm,aux_lat2,aux_lon2,aux_year,aux_month,aux_day\n",
    "    #del lon_storm,lat_storm,year_storm,month_storm,day_storm,hour_storm,lat,lon\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1143b3a-5143-45f4-92a0-6ca25723cee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b30913-873f-4165-b7ce-27902511038d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5039c7d1-c9a5-40e9-a172-b08abdff078d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4827c87-e448-4d48-be59-beafb4cc6ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb27874-8058-4592-9fd3-ed2f03c78e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36fe0a4-a6e4-4a96-a9c3-95457f48169c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-jupyter",
   "language": "python",
   "name": "myenv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
